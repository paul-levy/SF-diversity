{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive fit analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last access (y.m.d): 20.04.16  \n",
    "Last update (y.m.d): 20.04.16   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I gather all V1 cells (data, model, descriptive fits) and analyze based on this complete set.\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- Initialize  \n",
    "- Gather  \n",
    "- Distribution of tuning measures  \n",
    "  - model free measures  \n",
    "  - comparison with derived measures  \n",
    "  - comparison with Cavanuagh, other datasets  \n",
    "- Simple vs complex  \n",
    "- Tuning shifts  \n",
    "  - Preferred spatial frequency with contrast \n",
    "  - Center of mass with contrast \n",
    "  - SFBW with contrast \n",
    "  - sfVariance with contrast \n",
    "  - Trajectories of these metrics across contrasts  \n",
    "- Preferred spatial frequency with contrast *and* dispersion  \n",
    "  - Histograms/trajectories of metrics with dispersion, split by contrast  \n",
    "  - Median tuning measure shift across dispersion, plotted across all contrasts (together)  \n",
    "     - sfCom and pSf  \n",
    "     - sfCom and sfComCut (see figure for explanation)  \n",
    "  - Contrast shifts across dispersion  \n",
    "  - Plot shift distributions at each dispersion  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/plevy/.conda/envs/lcv-python/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/users/plevy/.conda/envs/lcv-python/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/users/plevy/.conda/envs/lcv-python/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/users/plevy/.conda/envs/lcv-python/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/users/plevy/.conda/envs/lcv-python/lib/python3.6/importlib/_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/users/plevy/.conda/envs/lcv-python/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import helper_fcns as hf\n",
    "# import model_responses as mr\n",
    "import scipy.stats as ss\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import ks_2samp, kstest, linregress\n",
    "import itertools\n",
    "import autoreload\n",
    "from IPython.core.display import display, HTML, Image\n",
    "\n",
    "import seaborn as sns\n",
    "# plt.style.use('https://raw.githubusercontent.com/paul-levy/SF_diversity/master/paul_plt_style.mplstyle');\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('https://raw.githubusercontent.com/paul-levy/SF_diversity/master/paul_plt_style.mplstyle');\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.backends.backend_pdf as pltSave\n",
    "%matplotlib inline\n",
    "plt.style.use('https://raw.githubusercontent.com/paul-levy/SF_diversity/master/paul_plt_style.mplstyle');\n",
    "\n",
    "# expDirs (and expNames must always be of the right length, i.e. specify for each expt dir \n",
    "## V1 version\n",
    "expDirs = ['V1_orig/', 'altExp/', 'V1/']\n",
    "expNames = ['dataList.npy', 'dataList.npy', 'dataList_glx.npy']\n",
    "\n",
    "nExpts = len(expDirs);\n",
    "\n",
    "# these are usually same for all expts...we'll \"tile\" below\n",
    "# fitBase = 'fitList_190502cA';\n",
    "fitBase = 'fitList_191023c';\n",
    "fitNamesWght = ['%s_wght_chiSq.npy' % fitBase];\n",
    "fitNamesFlat = ['%s_flat_chiSq.npy' % fitBase];\n",
    "####\n",
    "# descrFits - loss type determined by comparison (choose best; see modCompare.ipynb::Descriptive Fits)\n",
    "####\n",
    "dogNames = ['descrFits_190503_poiss_sach.npy', 'descrFits_190503_poiss_sach.npy', 'descrFits_191023_sach_sach.npy'];\n",
    "descrMod = 0; # which model for the diff. of gauss fits (0/1/2: flex/sach/tony)\n",
    "descrNames = ['descrFits_190503_sqrt_flex.npy', 'descrFits_190503_sqrt_flex.npy', 'descrFits_191023_sqrt_flex.npy'];\n",
    "\n",
    "rvcNames = ['rvcFits_191023_f0_NR.npy', 'rvcFits_191023_f0_NR.npy', 'rvcFits_191023_NR_pos.npy'];\n",
    "rvcMods = [1,1,1]; # 0-mov; 1-Nakarushton; 2-Peirce\n",
    "# rvcNames   = ['rvcFits_f0.npy'];\n",
    "# pack to easily tile\n",
    "expt = [expDirs, expNames, fitNamesWght, fitNamesFlat, descrNames, dogNames, rvcNames, rvcMods];\n",
    "for exp_i in range(len(expt)):\n",
    "    if len(expt[exp_i]) == 1:\n",
    "        expt[exp_i] = expt[exp_i] * nExpts;\n",
    "# now unpack for use\n",
    "expDirs, expNames, fitNamesWght, fitNamesFlat, descrNames, dogNames, rvcNames, rvcMods = expt;\n",
    "\n",
    "base_dir = os.getcwd() + '/';\n",
    "\n",
    "saveName = 'figures/reports/descrAnalysis_200316_V1/' # for one save name for all figures\n",
    "save_loc = base_dir + saveName;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before any plotting, fix plotting paramaters\n",
    "plt.style.use('https://raw.githubusercontent.com/paul-levy/SF_diversity/master/paul_plt_style.mplstyle');\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 20;\n",
    "rcParams['pdf.fonttype'] = 42 # should be 42, but there are kerning issues                                                                                                                                                                                                    \n",
    "rcParams['ps.fonttype'] = 42 # should be 42, but there are kerning issues                                                                                                                                                                                                     \n",
    "\n",
    "rcParams['lines.linewidth'] = 2.5;\n",
    "rcParams['lines.markeredgewidth'] = 0; # no edge, since weird tings happen then\n",
    "rcParams['axes.linewidth'] = 1.5;\n",
    "rcParams['lines.markersize'] = 5;\n",
    "\n",
    "rcParams['xtick.major.size'] = 15\n",
    "rcParams['xtick.minor.size'] = 8\n",
    "rcParams['ytick.major.size'] = 15\n",
    "rcParams['ytick.minor.size'] = 8\n",
    "\n",
    "rcParams['xtick.major.width'] = 5\n",
    "rcParams['xtick.minor.width'] = 2\n",
    "rcParams['ytick.major.width'] = 5\n",
    "rcParams['ytick.minor.width'] = 2\n",
    "\n",
    "rcParams['font.family'] = 'DejaVu Sans'\n",
    "rcParams['font.style'] = 'oblique';\n",
    "rcParams['font.size'] = 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather data\n",
    "\n",
    "Now, let's create a large list which will combine data/fits/analyses from all cells for all of the above experiments. For each cell, let's keep track of:\n",
    "* \"meta\" parameters:\n",
    "    * which experiment directory\n",
    "    * which dataList, fitList, descrFit, rvcFit\n",
    "    * which cell number within those lists\n",
    "    * which experiment index?\n",
    "    * which stimulus values (i.e. dispersions, sfs, contrasts)\n",
    "* inferred parameters:\n",
    "    * prefSf (for all conditions), along with % var explained\n",
    "    * c50 (again, for all conditions)\n",
    "* model parameters\n",
    "    * fit parameters, loss value for weighted and flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the jointList (will take ~20 minutes or more, if including V1_orig/ and altExp/)...or just load a pre-existing one (below)...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper_fcns' from '/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoreload.reload(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we want to track for each cell?\n",
    "jointList = []; # we'll pack dictionaries in a list...\n",
    "\n",
    "#### these are now defaults in hf.jl_create - but here, nonetheless, for reference!\n",
    "\n",
    "# any parameters we need for analysis below?\n",
    "varExplThresh = 65; # i.e. only include if the fit explains >X (e.g. 75)% variance\n",
    "dog_varExplThresh = 65; # i.e. only include if the fit explains >X (e.g. 75)% variance\n",
    "\n",
    "sf_range = [0.1, 10]; # allowed values of 'mu' for fits - see descr_fit.py for details\n",
    "\n",
    "conDig = 1; # i.e. round to nearest tenth (1 decimal place)\n",
    "rawInd = 0; # for accessing ratios/differences that we pass into diffsAtThirdCon\n",
    "\n",
    "muLoc = 2; # prefSF is in location '2' of parameter arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ExpoAnalysisTools/python/get_events.py:79: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if valInds == []:\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:545: DeprecationWarning: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
      "  binEdges = numpy.linspace(0, stimDur, 1+stimDur/binWidth);\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:3756: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if sf['f1f0_rat'] == []:\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:1239: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dog_norm = lambda f: dog(f) / norm;\n",
      "ExpoAnalysisTools/python/get_events.py:79: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if valInds == []:\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:3611: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  gauss_form = lambda extent, diam: np.square(const_term * np.trapz(np.exp(-np.square(np.divide(diam, extent))), x=diam));\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:3446: RuntimeWarning: overflow encountered in exp\n",
      "  vonMis = lambda w,xc,ds,theta: np.exp(np.cos(theta-xc)/w) + ds*np.exp(np.cos(theta-xc-np.pi)/w);\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:3448: RuntimeWarning: invalid value encountered in subtract\n",
      "  mod = lambda a,w,xc,ds,r0,theta: r0 + a*np.divide(vonMis(w,xc,ds,theta) - np.min(vonMis(w,xc,ds,theta)), vonMis(w,xc,ds,xc) - np.min(vonMis(w,xc,ds,theta)));\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:3448: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mod = lambda a,w,xc,ds,r0,theta: r0 + a*np.divide(vonMis(w,xc,ds,theta) - np.min(vonMis(w,xc,ds,theta)), vonMis(w,xc,ds,xc) - np.min(vonMis(w,xc,ds,theta)));\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:3446: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vonMis = lambda w,xc,ds,theta: np.exp(np.cos(theta-xc)/w) + ds*np.exp(np.cos(theta-xc-np.pi)/w);\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:3446: RuntimeWarning: invalid value encountered in multiply\n",
      "  vonMis = lambda w,xc,ds,theta: np.exp(np.cos(theta-xc)/w) + ds*np.exp(np.cos(theta-xc-np.pi)/w);\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:545: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  binEdges = numpy.linspace(0, stimDur, 1+stimDur/binWidth);\n",
      "/arc/2.2/p1/plevy/SF_diversity/sfDiv-OriModel/sfDiv-python/helper_fcns.py:1115: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if curr_amps == [] or curr_cons == []:\n",
      "/users/plevy/.conda/envs/lcv-python/lib/python3.6/site-packages/scipy/optimize/optimize.py:670: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: the real code for creating the jointList has been moved to helper_fcns!\n",
    "# WARNING: This takes [~/<]10 minutes (as of 20.04.14)\n",
    "# jointList_V1full = hf.jl_create(base_dir, [expDirs[-1]], [expNames[-1]], [fitNamesWght[-1]], [fitNamesFlat[-1]], [descrNames[-1]], [dogNames[-1]], [rvcNames[-1]], [rvcMods[-1]])\n",
    "\n",
    "jointList = hf.jl_create(base_dir, expDirs, expNames, fitNamesWght, fitNamesFlat, descrNames, dogNames, rvcNames, rvcMods, varExplThresh=varExplThresh, dog_varExplThresh=dog_varExplThresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# suffix = datetime.today().strftime('%y%m%d')\n",
    "\n",
    "# np.save(base_dir + 'jointList_V1full_%s' % suffix, jointList_V1full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use this if you want to run only jointList again only for V1/ but update the full list\n",
    "# inds_to_replace = np.arange(len(jointList) - len(jointList_V1full), len(jointList))\n",
    "# for i, ii in enumerate(inds_to_replace):\n",
    "#     jointList[ii] = jointList_V1full[i];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "suffix = datetime.today().strftime('%y%m%d')\n",
    "\n",
    "np.save(base_dir + 'jointList_%s_vT%d_dvT%d' % (suffix, varExplThresh, dog_varExplThresh), jointList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or just load the jointList you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'jointList_%s.npy' % suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '200414'\n",
    "try:\n",
    "    jointList = hf.np_smart_load(base_dir + 'jointList_%s.npy' % suffix)\n",
    "except: # if it wasn't pickled, then we'll need to just load without the .item which is implicit in hf.np_smart_load\n",
    "    jointList = np.load(base_dir + 'jointList_%s.npy' % suffix);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference of gaussian analysis\n",
    "\n",
    "quickly added on 19.11.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCells = len(jointList);\n",
    "\n",
    "allDisps = []; allCons = []; allSfs = []; allConByDisp =[]; #allSfRef = [];\n",
    "\n",
    "for cellNum in range(nCells):\n",
    "        \n",
    "    curr_meta = jointList[cellNum]['metadata'];\n",
    "        \n",
    "    allDisps.append(curr_meta['stimVals'][0])\n",
    "    allCons.append(curr_meta['stimVals'][1])\n",
    "    allSfs.append(curr_meta['stimVals'][2])\n",
    "    allConByDisp.append(curr_meta['val_con_by_disp']);\n",
    "#     # let's also save high contrast, single grating SF tuning\n",
    "#     allSfRef.append(sfRef);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK ON THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = [descrFits[x]['params'][0] for x in val_cells]\n",
    "flat_params = np.asarray(hf.flatten_list(all_params))\n",
    "all_varExpl = [descrFits[x]['varExpl'][0] for x in val_cells]\n",
    "flat_varExpl = np.asarray(hf.flatten_list(all_varExpl));\n",
    "all_prefSf = [descrFits[x]['prefSf'][0] for x in val_cells]\n",
    "flat_prefSf = np.asarray(hf.flatten_list(all_varExpl));\n",
    "all_NLL = [descrFits[x]['NLL'][0] for x in val_cells]\n",
    "flat_NLL = np.asarray(hf.flatten_list(all_NLL));\n",
    "\n",
    "n_cells = len(all_params);\n",
    "n_cons = np.array([len(allConByDisp[x][0]) for x in val_cells])\n",
    "max_nCons = np.max(n_cons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of tuning measures\n",
    "\n",
    "Here, let's look at the distribution of preferred spatial frequencies and bandwidths in the population.\n",
    "\n",
    "Suppose we are looking at the preferred spatial frequency measure. Located within metrics, the measure is organized by dispersion (low to high), and within that, by contrast (again, low to high). Thus, jointList[x]['metrics']['pSf'][0][6] would have the preferred spatial frequency zero dispersion, 6th highest (0-indexed) contrast. Thus, to get the highest contrast, we index \"-1\" for the contrast (single gratings, only, since with newer versions of the experiment, the total contrast values of different dispersions differ slightly, and therefore occupy different locations in the list of total contrasts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-free measures\n",
    "\n",
    "First, let's look at model-free measures of the spatial frequency tuning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = 0;\n",
    "con = -1; # highest for single gratings\n",
    "comHighCon = np.array([jointList[i]['metrics']['sfCom'][disp][con] for i in range(len(jointList))])\n",
    "comCutHighCon = np.array([jointList[i]['metrics']['sfComCut'][disp][con] for i in range(len(jointList))])\n",
    "varHighCon = np.array([jointList[i]['metrics']['sfVar'][disp][con] for i in range(len(jointList))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(20, 20))\n",
    "\n",
    "#######\n",
    "# central tendency\n",
    "#######\n",
    "bins = np.linspace(-1.5, 2.5, 13);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "coms_clipped = np.clip(hf.nan_rm(np.log2(comHighCon)), clipLow, clipHigh);\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "\n",
    "# calculate\n",
    "nSF = len(coms_clipped);\n",
    "medianSF = np.nanmedian(comHighCon);\n",
    "mnSF = np.nanmean(comHighCon);\n",
    "gmnSF = gmean(hf.nan_rm(comHighCon));\n",
    "\n",
    "# plot\n",
    "vals, _ = np.histogram(coms_clipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "ax[0].bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "tickStrs = ['%.2f' % np.power(2, x) for x in tickVals];\n",
    "tickStrs[0] = '<%s' % tickStrs[0];\n",
    "tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "ax[0].set_xticks(tickVals);\n",
    "ax[0].set_xticklabels(tickStrs, rotation=45);\n",
    "sns.despine(ax=ax[0], offset=5)\n",
    "\n",
    "ax[0].set_xlabel('center of mass sf (cpd)')\n",
    "ax[0].set_ylabel('fraction of cells')\n",
    "ax[0].set_title('frequency c.o.m. (V1, n=%d, median=%.2f, mn=%.2f, gmn=%.2f)' % (nSF, medianSF, mnSF, gmnSF));\n",
    "\n",
    "\n",
    "#######\n",
    "# variance\n",
    "#######\n",
    "bins = np.linspace(0.25, 1.75, 11);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "vars_clipped = np.clip(hf.nan_rm(varHighCon), clipLow, clipHigh);\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "\n",
    "# calculate\n",
    "nSF = len(vars_clipped);\n",
    "medianSF = np.nanmedian(varHighCon);\n",
    "mnSF = np.nanmean(varHighCon);\n",
    "gmnSF = gmean(hf.nan_rm(varHighCon));\n",
    "\n",
    "# plot\n",
    "vals, _ = np.histogram(vars_clipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "ax[1].bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "sns.despine(ax=ax[1], offset=5)\n",
    "tickStrs = ['%.2f' % np.power(2, x) for x in tickVals];\n",
    "tickStrs[0] = '<%s' % tickStrs[0];\n",
    "tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "ax[1].set_xticks(tickVals);\n",
    "ax[1].set_xticklabels(tickStrs);\n",
    "\n",
    "ax[1].set_xlabel('variance of tuning (a.u.)');\n",
    "ax[1].set_ylabel('fraction of cells');\n",
    "ax[1].set_title('variance (V1, n=%d, median=%.2f, mn=%.2f, gmn=%.2f)' % (nSF, medianSF, mnSF, gmnSF));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go to derived measures - i.e. measures of the tuning curve that rely on descriptive tuning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = 0;\n",
    "con = -1; # highest for single gratings\n",
    "pSfHighCon = np.array([jointList[i]['metrics']['pSf'][disp][con] for i in range(len(jointList))])\n",
    "bwHalfHighCon = np.array([jointList[i]['metrics']['bwHalf'][disp][con] for i in range(len(jointList))])\n",
    "bw34HighCon = np.array([jointList[i]['metrics']['bw34'][disp][con] for i in range(len(jointList))])\n",
    "\n",
    "dog_pSfHighCon = np.array([jointList[i]['metrics']['dog_pSf'][disp][con] for i in range(len(jointList))])\n",
    "dog_charFreqHighCon = np.array([jointList[i]['metrics']['dog_charFreq'][disp][con] for i in range(len(jointList))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare model-free and derived measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# central tendency\n",
    "ax[0].loglog(comHighCon, pSfHighCon, 'o', c='k');\n",
    "mn, mx = np.minimum(np.nanmin(comHighCon), np.nanmin(pSfHighCon)), np.maximum(np.nanmax(comHighCon), np.nanmax(pSfHighCon))\n",
    "ax[0].loglog([mn, mx], [mn, mx], 'k--');\n",
    "ax[0].set_xlabel('center of mass sf (cpd)')\n",
    "ax[0].set_ylabel('peak sf (cpd)')\n",
    "non_nan = np.logical_and(~np.isnan(comHighCon), ~np.isnan(pSfHighCon))\n",
    "slope, intercept, r, p, err = linregress(comHighCon[non_nan], pSfHighCon[non_nan])\n",
    "# slope, intercept, r, p, err = linregress(np.log(comHighCon[non_nan]), np.log(pSfHighCon[non_nan]))\n",
    "plt_vals = np.geomspace(mn, mx, 50);\n",
    "ax[0].loglog(plt_vals, intercept + plt_vals*slope, 'r-', label='m=%.2f, r=%.2f (p=%.1e)' % (slope, r, p))\n",
    "ax[0].legend(fontsize='small');\n",
    "sns.despine(ax=ax[0], offset=5)\n",
    "\n",
    "# variance\n",
    "ax[1].loglog(varHighCon, bwHalfHighCon, 'o', c='k');\n",
    "mn, mx = np.minimum(np.nanmin(varHighCon), np.nanmin(bwHalfHighCon)), np.maximum(np.nanmax(varHighCon), np.nanmax(bwHalfHighCon))\n",
    "ax[1].loglog([mn, mx], [mn, mx], 'k--');\n",
    "ax[1].set_xlabel('variance measure')\n",
    "ax[1].set_ylabel('bandwidth at half-height (oct)')\n",
    "non_nan = np.logical_and(~np.isnan(varHighCon), ~np.isnan(bwHalfHighCon))\n",
    "slope, intercept, r, p, err = linregress(varHighCon[non_nan], bwHalfHighCon[non_nan])\n",
    "# slope, intercept, r, p, err = linregress(np.log(varHighCon[non_nan]), np.log(bwHalfHighCon[non_nan]))\n",
    "plt_vals = np.geomspace(mn, mx, 50);\n",
    "ax[1].loglog(plt_vals, intercept + plt_vals*slope, 'r-', label='m=%.2f, r=%.2f (p=%.1e)' % (slope, r, p))\n",
    "ax[1].legend(fontsize='small');\n",
    "sns.despine(ax=ax[1], offset=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-2.5, 3.5, 13);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "\n",
    "psfs = [comHighCon, pSfHighCon];\n",
    "fitStr = ['c.o.m.', 'pSf']\n",
    "\n",
    "psfs_clipped = [np.clip(hf.nan_rm(np.log2(x)), clipLow, clipHigh) for x in psfs];\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "# tickVals = np.linspace(bins[1]+0.5*stepSize, bins[-2]+0.5*stepSize, 10)\n",
    "\n",
    "# calculate\n",
    "nSF = [len(x) for x in psfs_clipped]\n",
    "medianSF = [np.nanmedian(x) for x in psfs];\n",
    "mnSF = [np.nanmean(x) for x in psfs];\n",
    "gmnSF = [gmean(hf.nan_rm(x)) for x in psfs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(15, 2*10))\n",
    "# ax.set(xscale=\"log\")\n",
    "\n",
    "for i in range(2):\n",
    "\n",
    "    plt.subplot(2,1,1+i)\n",
    "    vals, _ = np.histogram(psfs_clipped[i], bins=bins)\n",
    "    vals_norm = vals/np.sum(vals)\n",
    "    plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "    sns.despine(offset=10)\n",
    "    tickStrs = ['%.2f' % np.power(2, x) for x in tickVals];\n",
    "    tickStrs[0] = '<%s' % tickStrs[0];\n",
    "    tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "    plt.xticks(tickVals, tickStrs);\n",
    "\n",
    "    plt.xlabel('spatial frequency (c/deg)');\n",
    "    plt.title('Distribution of frequency preference [%s] (V1, n=%d, median=%.2f, mn=%.2f, gmn=%.2f)' % (fitStr[i], nSF[i], medianSF[i], mnSF[i], gmnSF[i]));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cavanaugh dataset\n",
    "Now, let's load the Cavanaugh (?) dataset sent to me by Tony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_file = 'v1-jrc-cavanaugh.xls'\n",
    "df = pd.read_excel(cv_file);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the columns?\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### what do we want to get\n",
    "# first, get the restricted set\n",
    "eccBounds = [2, 6]\n",
    "ecc_match = np.logical_and(df['rfeccen']>eccBounds[0], df['rfeccen']<eccBounds[1])\n",
    "ecc_inds = np.where(ecc_match)[0]\n",
    "\n",
    "# pref Sf\n",
    "cav_psf = df['optsf'];\n",
    "cav_psfEcc = df['optsf'][ecc_inds]\n",
    "\n",
    "# sf BW\n",
    "cav_sfBw = df['sfwid'];\n",
    "cav_sfBwEcc = df['sfwid'][ecc_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cav: prefSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-2.5, 3.5, 13);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "\n",
    "psfs = [cav_psf, cav_psfEcc];\n",
    "fitStr = ['full', 'ecc-match']\n",
    "\n",
    "psfs_clipped = [np.clip(hf.nan_rm(np.log2(x)), clipLow, clipHigh) for x in psfs];\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "# tickVals = np.linspace(bins[1]+0.5*stepSize, bins[-2]+0.5*stepSize, 10)\n",
    "\n",
    "# calculate\n",
    "nSF = [len(x) for x in psfs_clipped]\n",
    "medianSF = [np.nanmedian(x) for x in psfs];\n",
    "mnSF = [np.nanmean(x) for x in psfs];\n",
    "gmnSF = [gmean(hf.nan_rm(x)) for x in psfs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(15, 2*10))\n",
    "# ax.set(xscale=\"log\")\n",
    "\n",
    "for i in range(2):\n",
    "\n",
    "    plt.subplot(2,1,1+i)\n",
    "    vals, _ = np.histogram(psfs_clipped[i], bins=bins)\n",
    "    vals_norm = vals/np.sum(vals)\n",
    "    plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "    sns.despine(offset=10)\n",
    "    tickStrs = ['%.2f' % np.power(2, x) for x in tickVals];\n",
    "    tickStrs[0] = '<%s' % tickStrs[0];\n",
    "    tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "    plt.xticks(tickVals, tickStrs);\n",
    "\n",
    "    plt.xlabel('preferred spatial frequency (c/deg)');\n",
    "    plt.title('[Cav] distribution of frequency preference [%s] (V1, n=%d, median=%.2f, mn=%.2f, gmn=%.2f)' % (fitStr[i], nSF[i], medianSF[i], mnSF[i], gmnSF[i]));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cav: bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we ask: are the bandwidths given in octaves or linear c/deg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ok = np.logical_and(~np.isnan(cav_psf), ~np.isnan(cav_sfBw))\n",
    "fit_mod = np.polyfit(cav_psf[all_ok], cav_sfBw[all_ok], deg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ok = np.logical_and(~np.isnan(cav_psf), ~np.isnan(cav_sfBw))\n",
    "# returns a, b in a*x + b\n",
    "fit_mod = np.polyfit(np.log2(cav_psf[all_ok]), cav_sfBw[all_ok], deg=1)\n",
    "\n",
    "plt.subplots(figsize=(8, 6))\n",
    "plt.plot(cav_psf, cav_sfBw, 'o')\n",
    "# plot the linear fit on log-lin (x-y) coords.\n",
    "all_sfs = np.geomspace(0.1, 10, 100);\n",
    "plt.plot(all_sfs, np.log2(all_sfs)*fit_mod[0] + fit_mod[1], 'k--')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('preferred spatial frequency (c/deg)')\n",
    "plt.ylabel('SF bandwidth (oct? c/deg?)')\n",
    "sns.despine(offset=10, trim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative lack of (or even slightly negative) correlation between preferred spatial frequency and bandwidth measure suggests that the latter measure is given in octaves, not linear cycles per degree. Why? Well, neurons with higher preferred SF have a larger __linear__ bandwidth, but generally __octave__ bandwidth remains constant as a function of preferred SF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 6, 13);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "\n",
    "sfBWs = [cav_sfBw, cav_sfBwEcc];\n",
    "fitStr = ['full', 'ecc-match']\n",
    "\n",
    "bw_clipped = [np.clip(hf.nan_rm(x), clipLow, clipHigh) for x in sfBWs];\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "\n",
    "# calculate\n",
    "nBW = [len(x) for x in bw_clipped]\n",
    "medianBW = [np.nanmedian(x) for x in sfBWs];\n",
    "mnBW = [np.nanmean(x) for x in sfBWs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(15, 2*10))\n",
    "# ax.set(xscale=\"log\")\n",
    "\n",
    "for i in range(2):\n",
    "\n",
    "    plt.subplot(2,1,1+i)\n",
    "    vals, _ = np.histogram(bw_clipped[i], bins=bins)\n",
    "    vals_norm = vals/np.sum(vals)\n",
    "    plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "    sns.despine(offset=10)\n",
    "    tickStrs = ['%.2f' % x for x in tickVals];\n",
    "    tickStrs[0] = '<%s' % tickStrs[0];\n",
    "    tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "    plt.xticks(tickVals, tickStrs);\n",
    "\n",
    "    plt.xlabel('spatial frequency bandwidth (oct)');\n",
    "    plt.title('Distribution of half-height bandwidth [%s] (V1, n=%d, median=%.2f, mn=%.2f)' % (fitStr[i], nBW[i], medianBW[i], mnBW[i]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (return to my data) Plot the distribution of preferred frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-2.5, 3.5, 13);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "\n",
    "psfs = [pSfHighCon, dog_pSfHighCon];\n",
    "fitStr = ['flex', 'DoG']\n",
    "\n",
    "psfs_clipped = [np.clip(hf.nan_rm(np.log2(x)), clipLow, clipHigh) for x in psfs];\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "# tickVals = np.linspace(bins[1]+0.5*stepSize, bins[-2]+0.5*stepSize, 10)\n",
    "\n",
    "# calculate\n",
    "nSF = [len(x) for x in psfs_clipped]\n",
    "medianSF = [np.nanmedian(x) for x in psfs];\n",
    "mnSF = [np.nanmean(x) for x in psfs];\n",
    "gmnSF = [gmean(hf.nan_rm(x)) for x in psfs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(15, 2*10))\n",
    "# ax.set(xscale=\"log\")\n",
    "\n",
    "for i in range(2):\n",
    "\n",
    "    plt.subplot(2,1,1+i)\n",
    "    vals, _ = np.histogram(psfs_clipped[i], bins=bins)\n",
    "    vals_norm = vals/np.sum(vals)\n",
    "    plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "    sns.despine(offset=10)\n",
    "    tickStrs = ['%.2f' % np.power(2, x) for x in tickVals];\n",
    "    tickStrs[0] = '<%s' % tickStrs[0];\n",
    "    tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "    plt.xticks(tickVals, tickStrs);\n",
    "\n",
    "    plt.xlabel('preferred spatial frequency (c/deg)');\n",
    "    plt.title('Distribution of frequency preference [%s] (V1, n=%d, median=%.2f, mn=%.2f, gmn=%.2f)' % (fitStr[i], nSF[i], medianSF[i], mnSF[i], gmnSF[i]));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to existing measurements:\n",
    "\n",
    "**DeValois, Albrecht, Thorell (1982):**\n",
    "Here, they split their data by X&Y, foveal ($[0^{\\circ}, 1.5^{\\circ}]$) and parafoveal ($[3^{\\circ}, 5^{\\circ}]$). Our data are parafoveal if not further, so we expect that our distribution of peak frequencies will be comparable to their X/Y parafoveal data, if not shifted lower, since we expect a fall-off in peak frequency with eccentricity (i.e., the highest peak frequency drops as a function of eccentricity, [DeValois & DeValois, 1980]). They have a mean of 2.2/3.2 cpd for Y/X parafoveal cells; our mean/gmean is 3.45/2.76 cpd. **Our cells are \"in the ballpark\".**\n",
    "\n",
    "<img src=\"figures/dv-sf.png\" width=400 />\n",
    "\n",
    "**Cavanaugh (intra-lab dataset):**\n",
    "See the above - his distribution has a substantially lower preferred spatial frequency (mean between 1.5 - 1.7 cpd, depending on full or eccentricity-matched dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution of characteristic frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.5, 4.5, 11);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "\n",
    "char_clipped = np.clip(hf.nan_rm(np.log2(dog_charFreqHighCon)), clipLow, clipHigh);\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "# tickVals = np.linspace(bins[1]+0.5*stepSize, bins[-2]+0.5*stepSize, 10)\n",
    "\n",
    "# calculate\n",
    "nCF = len(char_clipped)\n",
    "medianCF = np.nanmedian(dog_charFreqHighCon)\n",
    "mnCF = np.nanmean(dog_charFreqHighCon);\n",
    "gmnCF = gmean(hf.nan_rm(dog_charFreqHighCon));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "vals, _ = np.histogram(char_clipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "sns.despine(offset=10)\n",
    "tickStrs = ['%.2f' % np.power(2, x) for x in tickVals];\n",
    "tickStrs[0] = '<%s' % tickStrs[0];\n",
    "tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "plt.xticks(tickVals, tickStrs);\n",
    "\n",
    "plt.xlabel('characteristic spatial frequency (c/deg)');\n",
    "plt.title('Distribution of characteristic frequency (V1, n=%d, median=%.2f, mn=%.2f, gmn=%.2f)' % (nCF, medianCF, mnCF, gmnCF));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution of frequency tuning bandwidth:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, half-height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 6, 13);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "bwHalfClipped = np.clip(hf.nan_rm(bwHalfHighCon), clipLow, clipHigh)\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "\n",
    "# compute\n",
    "nBW = len(bwHalfClipped);\n",
    "medianBW = np.nanmedian(bwHalfHighCon);\n",
    "mnBW = np.nanmean(bwHalfHighCon);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 7))\n",
    "# ax.set(xscale=\"log\")\n",
    "\n",
    "vals, _ = np.histogram(bwHalfClipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "sns.despine(offset=10)\n",
    "tickStrs = ['%.2f' % x for x in tickVals];\n",
    "tickStrs[0] = '<%s' % tickStrs[0];\n",
    "tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "plt.xticks(tickVals, tickStrs);\n",
    "\n",
    "plt.xlabel('spatial frequency bandwidth (oct)');\n",
    "plt.title('Distribution of half-height bandwidth (V1, n=%d, median=%.2f, mn=%.2f)' % (nBW, medianBW, mnBW));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to existing measurements:\n",
    "\n",
    "**DeValois, Albrecht, Thorell (1982):**\n",
    "Here, they split their data by X&Y, foveal ($[0^{\\circ}, 1.5^{\\circ}]$) and parafoveal ($[3^{\\circ}, 5^{\\circ}]$). \n",
    "Our data are parafoveal if not further, so we expect that our distribution of bandwidth will be comparable to their X/Y parafoveal data. They have a mean of 1.6/1.32 octaves for Y/X parafoveal cells; our mean/median is 2.45/2.77 octaves. **Our cells are more broadly tuned. Why?**\n",
    "\n",
    "<img src=\"figures/dv-bw.png\" width=400 />\n",
    "\n",
    "**Cavanaugh (intra-lab dataset):**\n",
    "Our bandwidth measures are comparable (median/mean around 2.1/2.2 octaves), though ours are still broader. Note that we have assumed his measurements are given in octaves (and not linear CPD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, three-fourth height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 4, 13);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "clipLow, clipHigh = bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize;\n",
    "bw34Clipped = np.clip(hf.nan_rm(bw34HighCon), clipLow, clipHigh)\n",
    "tickVals = np.linspace(bins[1], bins[-2], len(bins)-2)\n",
    "\n",
    "# compute\n",
    "nBW = len(bw34Clipped);\n",
    "medianBW = np.nanmedian(bw34HighCon);\n",
    "mnBW = np.nanmean(bw34HighCon);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 7))\n",
    "vals, _ = np.histogram(bw34Clipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "sns.despine(offset=10)\n",
    "tickStrs = ['%.2f' % x for x in tickVals];\n",
    "tickStrs[0] = '<%s' % tickStrs[0];\n",
    "tickStrs[-1] = '>%s' % tickStrs[-1];\n",
    "plt.xticks(tickVals, tickStrs);\n",
    "\n",
    "plt.xlabel('spatial frequency bandwidth (oct)');\n",
    "plt.title('Distribution of 3/4-height bandwidth (V1, n=%d, median=%.2f, median=%.2f)' % (nBW, medianBW, mnBW));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relating fit quality to fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pSfHighCon = np.array([jointList[i]['metrics']['pSf'][disp][con] for i in range(len(jointList))])\n",
    "bwHalfHighCon = np.array([jointList[i]['metrics']['bwHalf'][disp][con] for i in range(len(jointList))])\n",
    "bw34HighCon = np.array([jointList[i]['metrics']['bw34'][disp][con] for i in range(len(jointList))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispInd, conInd = 0, -1;\n",
    "# for high contrast, single gratings\n",
    "descrVarExpl = [jointList[x]['metrics']['sfVarExpl'][dispInd][conInd] for x in range(len(jointList))]\n",
    "dogVarExpl = [jointList[x]['metrics']['dog_varExpl'][dispInd][conInd] for x in range(len(jointList))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMods, nComps = 2, 3;\n",
    "f, ax = plt.subplots(nMods, nComps, figsize=(nComps*7, nMods*8))\n",
    "\n",
    "allVar = [descrVarExpl, dogVarExpl];\n",
    "modStr = [hf.descrMod_name(0), hf.descrMod_name(1)]; # 0 [flex], 1 [sach], 2[tony]\n",
    "\n",
    "f.suptitle('Do the descriptive fits fail with particular tuning properties?')\n",
    "\n",
    "for i in range(len(allVar)):\n",
    "\n",
    "    ### compute the correlations\n",
    "    jointNan = np.logical_or(np.isnan(allVar[i]), np.isnan(pSfHighCon));\n",
    "    r = np.corrcoef(np.array(allVar[i])[~jointNan], pSfHighCon[~jointNan])\n",
    "    rSf = r[0, 1]; # [0,1] and [1,0] are the same - corr. b/t \"x\" and \"y\"\n",
    "    # bwHalf\n",
    "    jointNan = np.logical_or(np.isnan(allVar[i]), np.isnan(bwHalfHighCon));\n",
    "    r = np.corrcoef(np.array(allVar[i])[~jointNan], bwHalfHighCon[~jointNan])\n",
    "    rBw = r[0, 1]; # [0,1] and [1,0] are the same - corr. b/t \"x\" and \"y\"\n",
    "    # bw34\n",
    "    jointNan = np.logical_or(np.isnan(allVar[i]), np.isnan(bw34HighCon));\n",
    "    r = np.corrcoef(np.array(allVar[i])[~jointNan], bw34HighCon[~jointNan])\n",
    "    rBw34 = r[0, 1]; # [0,1] and [1,0] are the same - corr. b/t \"x\" and \"y\"\n",
    "    \n",
    "    # first, varExpl vs. prefSf\n",
    "    ax[i, 0].semilogy(allVar[i], pSfHighCon, 'o')\n",
    "    ax[i, 0].set_xlabel('% variance explained')\n",
    "    ax[i, 0].set_ylabel('preferred SF (c/deg)')\n",
    "    ax[i, 0].set_title('%s model (r=%.2f)' %  (modStr[i], rSf));\n",
    "    # then, varExpl vs. sfBw (1/2)\n",
    "    ax[i, 1].plot(allVar[i], bwHalfHighCon, 'o')\n",
    "    ax[i, 1].set_xlabel('% variance explained')\n",
    "    ax[i, 1].set_ylabel('bandwidth (1/2; oct)')\n",
    "    ax[i, 1].set_title('(r=%.2f)' % rBw)\n",
    "    # finally, varExpl vs. sfBw (3/4)\n",
    "    ax[i, 2].plot(allVar[i], bw34HighCon, 'o')\n",
    "    ax[i, 2].set_xlabel('% variance explained')\n",
    "    ax[i, 2].set_ylabel('bandwidth (3/4; oct)')\n",
    "    ax[i, 2].set_title('(r=%.2f)' % rBw34)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple vs complex\n",
    "How do the tuning properties above vary with F1/F0 ratio? Here, I compute the F1/F0 ratio based on the responses to the optimial single grating SF at high contrast. I take the F0 and F1 values for each repeat of that condition, and compute the mean of all individual trial ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCells = len(jointList)\n",
    "f1f0_rats = np.array([jointList[i]['metrics']['f1f0_ratio'] for i in range(nCells)])\n",
    "f1f0_saveName = 'simple_complex_distr_UPDATE.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRows = 3\n",
    "f, ax = plt.subplots(nRows, 1, figsize=(1*7, nRows*10))\n",
    "\n",
    "nS, nC = sum(f1f0_rats>1), sum(f1f0_rats<1)\n",
    "ax[0].hist(hf.nan_rm(f1f0_rats), bins=11, label='%d(s), %d(c)' % (nS, nC));\n",
    "ax[0].axvline(1.0, ls='--', color='k');\n",
    "ax[0].set_title('F1::F0 ratio - linear scale')\n",
    "ax[0].legend();\n",
    "\n",
    "fracS = nS/(nS+nC);\n",
    "bins = ax[1].hist(hf.nan_rm(np.log10(f1f0_rats)), bins=9, label='%.1f%% simple' % (100*fracS))[1];\n",
    "ax[1].axvline(0, ls='--', color='k');\n",
    "ax[1].set_title('F1::F0 ratio - log10 scale')\n",
    "ax[1].legend();\n",
    "\n",
    "# NOTE: Cavanaugh's F1F0 ratio seems to be in the 'complexity' field, and is already log(base10) transformed\n",
    "cav_complexity = hf.nan_rm(df['complexity'])\n",
    "c_nS, c_nC = sum(cav_complexity>0), sum(cav_complexity<0)\n",
    "ax[2].hist(cav_complexity, bins=bins, label='%.1f%% simple' % (100*(c_nS/(c_nS+c_nC))));\n",
    "ax[2].axvline(0, ls='--', color='k');\n",
    "ax[2].set_title('F1::F0 ratio - Cavanaugh')\n",
    "ax[2].legend();\n",
    "\n",
    "sns.despine(offset=10)\n",
    "\n",
    "# now save\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "pdfSv = pltSave.PdfPages(save_loc + f1f0_saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nRows = 3\n",
    "f, ax = plt.subplots(nRows, 1, figsize=(1*7, nRows*10))\n",
    "\n",
    "nS, nC = sum(f1f0_rats>1), sum(f1f0_rats<1)\n",
    "ax[0].hist(hf.nan_rm(f1f0_rats), bins=11, label='%d(s), %d(c)' % (nS, nC));\n",
    "ax[0].axvline(1.0, ls='--', color='k');\n",
    "ax[0].set_title('F1::F0 ratio - linear scale')\n",
    "ax[0].legend();\n",
    "\n",
    "fracS = nS/(nS+nC);\n",
    "bins = ax[1].hist(hf.nan_rm(np.log10(f1f0_rats)), bins=7, label='%.1f%% simple' % (100*fracS))[1];\n",
    "ax[1].axvline(0, ls='--', color='k');\n",
    "ax[1].set_title('F1::F0 ratio - log10 scale')\n",
    "ax[1].legend();\n",
    "\n",
    "# NOTE: Cavanaugh's F1F0 ratio seems to be in complexity, and is already log(base10) transformed\n",
    "cav_complexity = hf.nan_rm(df['complexity'])\n",
    "c_nS, c_nC = sum(cav_complexity>0), sum(cav_complexity<0)\n",
    "ax[2].hist(cav_complexity, bins=bins, label='%.1f%% simple' % (100*(c_nS/(c_nS+c_nC))));\n",
    "ax[2].axvline(0, ls='--', color='k');\n",
    "ax[2].set_title('F1::F0 ratio - Cavanaugh')\n",
    "ax[2].legend();\n",
    "\n",
    "sns.despine(offset=10)\n",
    "\n",
    "# now save\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "pdfSv = pltSave.PdfPages(save_loc + f1f0_saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, we can see that the f1/f0 ratios in my V1 dataset are comparable to Cavanaugh's, both in \"sign\" and magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1f0tuning_name = 'f1f0_tuning.pdf';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using for colors\n",
    "zTO1 = lambda x: np.divide(x - np.nanmin(x), np.nanmax(x - np.nanmin(x)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCols = 2; # each of the below measures will have two types to analyze\n",
    "nMeasure = 2; # central tendency of SF, variance of S\n",
    "\n",
    "f, ax = plt.subplots(nMeasure, nCols, figsize=(10*nCols, 10*nMeasure));\n",
    "# we'll color each point according to the log10'd f1f0 ratio (this scale is more uniform)\n",
    "log10_f1f0rats = np.log10(f1f0_rats);\n",
    "clrs = cm.gray(np.ones_like(log10_f1f0rats) - 0.75);\n",
    "lt0 = np.where(log10_f1f0rats < 0)[0];\n",
    "clrs[lt0, :] = cm.gray(np.ones_like(lt0) - 0.25);\n",
    "# clrs = cm.gray(zTO1(log10_f1f0rats));\n",
    "\n",
    "# first, C.o.M. and pSF\n",
    "metrics = [comHighCon, pSfHighCon];\n",
    "labels = ['center of mass', 'preferred SF'];\n",
    "for ind, (m, l) in enumerate(zip(metrics, labels)):\n",
    "    ax[0, ind].scatter(m, log10_f1f0rats, color=clrs)\n",
    "    ax[0, ind].set_ylabel('log10 f1f0 ratio');\n",
    "    ax[0, ind].set_xlabel('spatial frequency (c/deg)');\n",
    "    ax[0, ind].set_title('%s' % l)\n",
    "    ax[0, ind].set_xscale('log')\n",
    "\n",
    "# second, sfVar and bw34\n",
    "metrics = [varHighCon, bwHalfHighCon];\n",
    "labels = ['sfVar', 'bw1/2 (oct)'];\n",
    "for ind, (m, l) in enumerate(zip(metrics, labels)):\n",
    "    ax[1, ind].scatter(m, log10_f1f0rats, color=clrs)\n",
    "    ax[1, ind].set_ylabel('log10 f1f0 ratio');\n",
    "    ax[1, ind].set_xlabel('%s' % l);\n",
    "    ax[1, ind].set_title('tuning curve breadth')\n",
    "    \n",
    "sns.despine(offset=10)\n",
    "\n",
    "# now save\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "pdfSv = pltSave.PdfPages(save_loc + f1f0tuning_name);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per a discussion with MJH on 20.03.26, he said that there was substantial overlap in the preferred SF of simple and complex cells, but a small group of simple cells had (MAYBE) lower preferred SF than was typically seen in complex cells, and somewhat lower bandwidths. The preferred frequency observation holds here, though bandwidth is less distinct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifts in tuning with contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferred spatial frequency with contrast inferred from descriptive fit to pSf vs. con\n",
    "\n",
    "Now, we plot the ratio of preferred spatial frequencies in the data set. Here, we are using \"pSfModRat\" - a derived measure in which we fit a simple polynomial equation to the preferred frequencies as a function of contrast (see descr_fits.py for details).\n",
    "\n",
    "- In [0, 0], the ratio of preferred spatial frequencies ($\\frac{con_a}{con_b}$) evaluated at the extrema contrasts\n",
    "- In [0, 1], the ratio of preferred spatial frequencies is expressed per unit log2 contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = 'sfAllConRatio_%s.pdf' % fitBase\n",
    "\n",
    "# [0, 0] for single gratings, ratio\n",
    "# [0, 1] for single gratings, ratio per unit log2 contrast\n",
    "psfModRats = [jointList[i]['metrics']['pSfModRat'][0, 1] for i in range(len(jointList))];\n",
    "psfModRats = np.array(psfModRats);\n",
    "\n",
    "bins = np.linspace(-1.25, 1.25, 11);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "psfClipped = np.clip(psfModRats, bins[1]-0.5*stepSize, bins[-2]+0.5*stepSize)\n",
    "\n",
    "tickVals = np.linspace(bins[1], bins[-2], 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "\n",
    "vals, _ = np.histogram(psfClipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "mean, median = [np.nanmean(psfModRats), np.nanmedian(psfModRats)];\n",
    "std = np.nanstd(psfModRats);\n",
    "stdAsRatio = std/np.power(2, mean);\n",
    "plt.plot(mean, 1.25*np.max(vals_norm), 'v', label='mean (%.2f) +/- %.0f%%' % (np.power(2, mean), 100*stdAsRatio));\n",
    "plt.plot(median, 1.25*np.max(vals_norm), 'v', label='median (%.2f)' % np.power(2, median));\n",
    "plt.axvline(0, ls='--', c='k', label='no effect')\n",
    "plt.legend(fontsize='x-small');\n",
    "plt.xticks(tickVals, ['%.2f' % np.power(2, x) for x in tickVals]);\n",
    "plt.xlabel('prefSF ratio per log2 contrast');\n",
    "plt.title('Preferred SF changes with contrast (n=%d)' % sum(vals), fontsize='small');\n",
    "sns.despine(offset=10)\n",
    "\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...aside: suppose want to look at cells where the ratio shift is in the top 25% (i.e. highest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rm = lambda x: x[~np.isnan(x)]\n",
    "\n",
    "prctile = 75;\n",
    "thresh = np.percentile(nan_rm(psfModRats), prctile)\n",
    "# thresh = np.percentile(np.abs(nan_rm(psfRats)), prctile)\n",
    "shiftCells = np.where(psfModRats>thresh)[0]\n",
    "\n",
    "allChiDiffs = [];\n",
    "\n",
    "for i in shiftCells:\n",
    "    currEntry = jointList[i];\n",
    "    currName = currEntry['metadata']['expName'];\n",
    "    try:\n",
    "        flat, wght = currEntry['model']['NLL_flat'], currEntry['model']['NLL_wght'];\n",
    "        chiSqDiff = flat-wght;\n",
    "        allChiDiffs.append(chiSqDiff);\n",
    "        print('%s: flat|wght is (%.2f|%.2f)' % (currName, flat, wght));\n",
    "    except: # no fit for this cell\n",
    "        pass\n",
    "allChiDiffs = np.array(allChiDiffs);\n",
    "    \n",
    "sns.distplot(allChiDiffs[~np.isnan(allChiDiffs)], kde=False);\n",
    "sns.despine(offset=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferred spatial frequency with contrast from data, 1:.33 contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = 'sfFixConRatioNorm_%s.pdf' % fitBase;\n",
    "\n",
    "# get psfRatio 1:.33 contrast (raw value, not norm. to con value)\n",
    "disp = 0; # i.e. single gratings\n",
    "toNorm = 0;\n",
    "if toNorm == 1:\n",
    "    conNorm = np.log2(1/0.33);\n",
    "else:\n",
    "    conNorm = 1;\n",
    "    \n",
    "# why ['diffsAtThirdCon'][2] - [2] is pSf\n",
    "psfRats = [jointList[i]['metrics']['diffsAtThirdCon'][disp, 2]/conNorm for i in range(len(jointList))];\n",
    "psfRats = np.array(psfRats);\n",
    "\n",
    "bins = np.linspace(-1.25, 1.25, 11);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "psfClipped = np.clip(psfRats, bins[0]+0.5*stepSize, bins[-1]-0.5*stepSize)\n",
    "\n",
    "tickVals = np.linspace(bins[1], bins[-2], 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "\n",
    "vals, _ = np.histogram(psfClipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "mean, median = [np.nanmean(psfRats), np.nanmedian(psfRats)];\n",
    "std = np.nanstd(psfRats);\n",
    "stdAsRatio = std/np.power(2, mean);\n",
    "plt.plot(mean, 1.25*np.max(vals_norm), 'v', label='mean (%.2f) +/- %.0f%%' % (np.power(2, mean), 100*stdAsRatio));\n",
    "plt.plot(median, 1.25*np.max(vals_norm), 'v', label='median (%.2f)' % np.power(2, median));\n",
    "plt.axvline(0, ls='--', c='k', label='no effect')\n",
    "plt.legend(fontsize='x-small', loc='upper left');\n",
    "plt.xticks(tickVals, ['%.2f' % np.power(2, x) for x in tickVals]);\n",
    "plt.xlabel('prefSF ratio (1::0.33 contrast)');\n",
    "plt.title('Preferred SF changes with contrast (n=%d)' % sum(vals), fontsize='small');\n",
    "sns.despine(offset=10)\n",
    "\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are these differences significant?\n",
    "The way to test this is a paired t-test, since want to know if the two samples of the metric (e.g. preferred frequency at two different contrasts) have a different mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highCon = hf.jl_get_metric_byCon(jointList, 'pSf', conVal=1.0, disp=0)\n",
    "lowCon  = hf.jl_get_metric_byCon(jointList, 'pSf', conVal=0.33, disp=0)\n",
    "non_nan = np.logical_and(~np.isnan(highCon), ~np.isnan(lowCon));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_t = ss.ttest_rel(highCon[non_nan], lowCon[non_nan])\n",
    "pval = paired_t.pvalue;\n",
    "if pval <= 0.05:\n",
    "    print('significant! p=%.2e (n=%d)' % (pval, sum(non_nan)));\n",
    "elif paired_t.pvalue > 0.05:\n",
    "    print('not significant! p=%.2e (n=%d)' % (pval, sum(non_nan)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But now with center of mass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get psfRatio 1:.33 contrast (raw value, not norm. to con value)\n",
    "disp = 0; # i.e. single gratings\n",
    "\n",
    "toNorm = 0;\n",
    "if toNorm == 1:\n",
    "    conNorm = np.log2(1/0.33);\n",
    "    normStr = 'Norm'\n",
    "else:\n",
    "    conNorm = 1;\n",
    "    normStr = ''\n",
    "\n",
    "saveName = 'sfComFixConRatio%s_%s.pdf' % (normStr, fitBase);\n",
    "    \n",
    "# why ['diffsAtThirdCon'][4] - [4] is center of mass ratio\n",
    "comRats = [jointList[i]['metrics']['diffsAtThirdCon'][disp, 4]/conNorm for i in range(len(jointList))];\n",
    "comRats = np.array(comRats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(12, 2*12))\n",
    "\n",
    "### same scale as pSf above\n",
    "bins = np.linspace(-1.25, 1.25, 11);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "comClipped = np.clip(comRats, bins[0]+0.5*stepSize, bins[-1]-0.5*stepSize)\n",
    "\n",
    "tickVals = np.linspace(bins[1], bins[-2], 5);\n",
    "\n",
    "vals, _ = np.histogram(comClipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "ax[0].bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "mean, median = [np.nanmean(comRats), np.nanmedian(comRats)];\n",
    "std = np.nanstd(comRats);\n",
    "stdAsRatio = std/np.power(2, mean);\n",
    "ax[0].plot(mean, 1.25*np.max(vals_norm), 'v', markersize=10, label='mean (%.2f) +/- %.0f%%' % (np.power(2, mean), 100*stdAsRatio));\n",
    "ax[0].plot(median, 1.25*np.max(vals_norm), 'v', markersize=10, label='median (%.2f)' % np.power(2, median));\n",
    "ax[0].axvline(0, ls='--', c='k', label='no effect')\n",
    "ax[0].legend(fontsize='x-small', loc='upper left');\n",
    "ax[0].set_xticks(tickVals)\n",
    "ax[0].set_xticklabels(['%.2f' % np.power(2, x) for x in tickVals]);\n",
    "ax[0].set_xlabel('c.o.m. ratio (1::0.33 contrast)');\n",
    "ax[0].set_title('Center-of-mass changes with contrast (n=%d)' % sum(vals), fontsize='small');\n",
    "sns.despine(offset=10)\n",
    "\n",
    "### better scale given range of c.o.m.\n",
    "bins = np.linspace(-0.5, 0.5, 9);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "comClipped = np.clip(comRats, bins[0]+0.5*stepSize, bins[-1]-0.5*stepSize)\n",
    "\n",
    "tickVals = np.linspace(bins[1], bins[-2], 5);\n",
    "\n",
    "vals, _ = np.histogram(comClipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "ax[1].bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "mean, median = [np.nanmean(comRats), np.nanmedian(comRats)];\n",
    "std = np.nanstd(comRats);\n",
    "stdAsRatio = std/np.power(2, mean);\n",
    "ax[1].plot(mean, 1.25*np.max(vals_norm), 'v', markersize=10, label='mean (%.2f) +/- %.0f%%' % (np.power(2, mean), 100*stdAsRatio));\n",
    "ax[1].plot(median, 1.25*np.max(vals_norm), 'v', markersize=10, label='median (%.2f)' % np.power(2, median));\n",
    "ax[1].axvline(0, ls='--', c='k', label='no effect')\n",
    "ax[1].legend(fontsize='x-small', loc='upper left');\n",
    "ax[1].set_xticks(tickVals);\n",
    "ax[1].set_xticklabels(['%.2f' % np.power(2, x) for x in tickVals]);\n",
    "ax[1].set_xlabel('c.o.m. ratio (1::0.33 contrast)');\n",
    "ax[1].set_title('Center-of-mass changes with contrast (n=%d)' % sum(vals), fontsize='small');\n",
    "sns.despine(offset=10)\n",
    "\n",
    "\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are these differences significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highCon = hf.jl_get_metric_byCon(jointList, 'sfCom', conVal=1.0, disp=0)\n",
    "lowCon  = hf.jl_get_metric_byCon(jointList, 'sfCom', conVal=0.33, disp=0)\n",
    "non_nan = np.logical_and(~np.isnan(highCon), ~np.isnan(lowCon));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_t = ss.ttest_rel(highCon[non_nan], lowCon[non_nan])\n",
    "pval = paired_t.pvalue;\n",
    "if pval <= 0.05:\n",
    "    print('significant! p=%.2e (n=%d)' % (pval, sum(non_nan)));\n",
    "elif paired_t.pvalue > 0.05:\n",
    "    print('not significant! p=%.2e (n=%d)' % (pval, sum(non_nan)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandwidth with contrast from data, 1:.33 contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBase = lambda s: 'bwFixedCons%s_%s.pdf' % (s, fitBase);\n",
    "\n",
    "disp = 0; # looking at single gratings\n",
    "# get bw diff 1:.33 contrast (raw value, not norm. to con value)\n",
    "bwAt = 0; # 0 - half-height; 1 - 3/4 height\n",
    "\n",
    "bwStr = ['1/2 ht', '3/4 ht'];\n",
    "bwSaveStr = ['_hh', '_3qh'];\n",
    "\n",
    "bwDiff = [jointList[i]['metrics']['diffsAtThirdCon'][disp, bwAt] for i in range(len(jointList))];\n",
    "bwDiff = np.array(bwDiff);\n",
    "\n",
    "bins = np.linspace(-2.5, 2.5, 11);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "bwClipped = np.clip(bwDiff, bins[0]+0.5*stepSize, bins[-1]-0.5*stepSize)\n",
    "\n",
    "tickVals = np.linspace(bins[1], bins[-2], 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(bwDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "\n",
    "vals, _ = np.histogram(bwClipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "plt.bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "mean, median = [np.nanmean(bwDiff), np.nanmedian(bwDiff)];\n",
    "std = np.nanstd(bwClipped);\n",
    "# stdAsRatio = 0\n",
    "stdAsRatio = std/mean;\n",
    "plt.plot(mean, 1.25*np.max(vals_norm), 'v', markersize=10, label='mean (%.2f) +/- %.0f%%' % (mean, 100*stdAsRatio));\n",
    "plt.plot(median, 1.25*np.max(vals_norm), 'v', markersize=10, label='median (%.2f)' % median);\n",
    "plt.axvline(0, ls='--', c='k', label='no effect')\n",
    "plt.legend(fontsize='x-small');\n",
    "plt.xlabel('octave bandwidth difference (1::0.33 contrast)');\n",
    "plt.title('Bandwidth SF changes with contrast (n=%d, %s)' % (sum(vals), bwStr[bwAt]), fontsize='small');\n",
    "sns.despine(offset=10)\n",
    "\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "saveName = saveBase(bwSaveStr[bwAt])\n",
    "pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for significance\n",
    "metr = ['bwHalf', 'bw34']\n",
    "\n",
    "for m in metr:\n",
    "\n",
    "    highCon = hf.jl_get_metric_byCon(jointList, m, conVal=1.0, disp=0)\n",
    "    lowCon  = hf.jl_get_metric_byCon(jointList, m, conVal=0.33, disp=0)\n",
    "    non_nan = np.logical_and(~np.isnan(highCon), ~np.isnan(lowCon));\n",
    "\n",
    "    paired_t = ss.ttest_rel(highCon[non_nan], lowCon[non_nan])\n",
    "    pval = paired_t.pvalue;\n",
    "    if pval <= 0.05:\n",
    "        print('%s: significant! p=%.2e (n=%d)' % (m, pval, sum(non_nan)));\n",
    "    elif paired_t.pvalue > 0.05:\n",
    "        print('%s not significant! p=%.2e (n=%d)' % (m, pval, sum(non_nan)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with model-free metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBase = lambda s: 'bwVarFixedCons_%s.pdf' % (fitBase);\n",
    "\n",
    "disp = 0;\n",
    "# why ['diffsAtThirdCon'][3]? [3] is model-free variance measure\n",
    "varDiff = [jointList[i]['metrics']['diffsAtThirdCon'][disp, 3] for i in range(len(jointList))];\n",
    "varDiff = np.array(varDiff);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(12, 2*12))\n",
    "\n",
    "### first, same scale as above\n",
    "\n",
    "bins = np.linspace(-2.5, 2.5, 11);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "varClipped = np.clip(varDiff, bins[0]+0.5*stepSize, bins[-1]-0.5*stepSize)\n",
    "\n",
    "tickVals = np.linspace(bins[1], bins[-2], 5);\n",
    "\n",
    "vals, _ = np.histogram(varClipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "ax[0].bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "mean, median = [np.nanmean(varDiff), np.nanmedian(varDiff)];\n",
    "std = np.nanstd(varClipped);\n",
    "# stdAsRatio = 0\n",
    "stdAsRatio = std/mean;\n",
    "ax[0].plot(mean, 1.25*np.max(vals_norm), 'v', label='mean (%.2f) +/- %.0f%%' % (mean, 100*stdAsRatio));\n",
    "ax[0].plot(median, 1.25*np.max(vals_norm), 'v', label='median (%.2f)' % median);\n",
    "ax[0].axvline(0, ls='--', c='k', label='no effect')\n",
    "ax[0].legend(fontsize='x-small');\n",
    "ax[0].set_xlabel('sfVar difference (1::0.33 contrast)');\n",
    "ax[0].set_title('SF variance changes with contrast (n=%d)' % (sum(vals)), fontsize='small');\n",
    "sns.despine(offset=10)\n",
    "\n",
    "### then, better scale for this metric\n",
    "\n",
    "bins = np.linspace(-0.5, 0.5, 11);\n",
    "stepSize = (bins[-1] - bins[0])/(len(bins)-1);\n",
    "binPlotLocs = bins[1:] - 0.5*stepSize;\n",
    "varClipped = np.clip(varDiff, bins[0]+0.5*stepSize, bins[-1]-0.5*stepSize)\n",
    "\n",
    "tickVals = np.linspace(bins[1], bins[-2], 5);\n",
    "\n",
    "vals, _ = np.histogram(varClipped, bins=bins)\n",
    "vals_norm = vals/np.sum(vals)\n",
    "ax[1].bar(binPlotLocs, vals_norm, width=0.8*stepSize, align='center');\n",
    "mean, median = [np.nanmean(varDiff), np.nanmedian(varDiff)];\n",
    "std = np.nanstd(varClipped);\n",
    "# stdAsRatio = 0\n",
    "stdAsRatio = std/mean;\n",
    "ax[1].plot(mean, 1.25*np.max(vals_norm), 'v', label='mean (%.2f) +/- %.0f%%' % (mean, 100*stdAsRatio));\n",
    "ax[1].plot(median, 1.25*np.max(vals_norm), 'v', label='median (%.2f)' % median);\n",
    "ax[1].axvline(0, ls='--', c='k', label='no effect')\n",
    "ax[1].legend(fontsize='x-small');\n",
    "ax[1].set_xlabel('sfVar difference (1::0.33 contrast)');\n",
    "ax[1].set_title('SF variance changes with contrast (n=%d)' % (sum(vals)), fontsize='small');\n",
    "sns.despine(offset=10)\n",
    "\n",
    "### now save\n",
    "\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "saveName = saveBase(bwSaveStr)\n",
    "pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for significance\n",
    "m = 'sfVar'\n",
    "\n",
    "highCon = hf.jl_get_metric_byCon(jointList, m, conVal=1.0, disp=0)\n",
    "lowCon  = hf.jl_get_metric_byCon(jointList, m, conVal=0.33, disp=0)\n",
    "non_nan = np.logical_and(~np.isnan(highCon), ~np.isnan(lowCon));\n",
    "\n",
    "paired_t = ss.ttest_rel(highCon[non_nan], lowCon[non_nan])\n",
    "pval = paired_t.pvalue;\n",
    "if pval <= 0.05:\n",
    "    print('%s: significant! p=%.2e (n=%d)' % (m, pval, sum(non_nan)));\n",
    "elif paired_t.pvalue > 0.05:\n",
    "    print('%s not significant! p=%.2e (n=%d)' % (m, pval, sum(non_nan)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the trajectory of these measures across all contrasts\n",
    "Rather than simply analyzing the ratio at any two contrasts, let's consider the evolution of these measures across all contrasts (for a given cell and dispersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# First, decide which measure we're using; prepare plot\n",
    "#######\n",
    "subset = ['all', 'V1orig', 'V1new']\n",
    "\n",
    "sfMetrics = ['sfCom', 'pSf', 'bwHalf'];\n",
    "nMetrics = len(sfMetrics);\n",
    "metricColor = ['k', 'b', 'g']\n",
    "\n",
    "# allCon_sf  = dict();\n",
    "# allCon_con = dict();\n",
    "\n",
    "nCells = len(jointList);\n",
    "\n",
    "maxCons = np.max([len(jointList[i]['metadata']['stimVals'][1]) for i in range(nCells)]); # max nCons\n",
    "# dispColor = cm.rainbow(np.linspace(0, 1, maxDisp));\n",
    "\n",
    "for mtr, mtrClr in zip(sfMetrics, metricColor):\n",
    "\n",
    "    for sub_ind, sbst in zip(range(len(subset)), subset):\n",
    "    \n",
    "        saveName = 'sfTune_conTrends_%s_%s.pdf' % (mtr, sbst)\n",
    "\n",
    "        #######\n",
    "        # subset the data\n",
    "        #######\n",
    "        \n",
    "        # first, determine which cells we'll consider\n",
    "        expInds = np.array([jointList[i]['metadata']['expInd'] for i in range(nCells)])\n",
    "        if sub_ind == 0: # all data\n",
    "            # NOTE: yes, we could just grab 'median', but want to track # cells\n",
    "            ok = np.where(expInds > 0)[0]; # i.e. any experiment...\n",
    "        elif sub_ind == 1: # V1_orig\n",
    "            ok = np.where(expInds == 1)[0]\n",
    "        elif sub_ind == 2: # V1\n",
    "            ok = np.where(expInds != 1)[0]\n",
    "    \n",
    "        # how to handle contrast? for each cell, find the contrast that is valid for that dispersion and matches the con_lvl\n",
    "        # to within some tolerance (e.g. +/- 0.01, i.e. 1% contrast)\n",
    "\n",
    "        #######\n",
    "        # set up plot, analyses\n",
    "        #######\n",
    "        maxDisp = np.max([len(jointList[i]['metadata']['stimVals'][0]) for i in ok]) # 0 for disps...\n",
    "        dispColor = cm.rainbow(np.linspace(0, 1, maxDisp));\n",
    "        f, ax = plt.subplots(maxDisp, 2, figsize=(2*20, nMetrics*20));\n",
    "\n",
    "        full_slope = [];\n",
    "        prev_ahh = None; # store previous median slope value\n",
    "\n",
    "    #     allDisp_sf[con_ind] = dict();\n",
    "    \n",
    "        for d, clr in zip(range(maxDisp), dispColor):\n",
    "\n",
    "            all_sfs = [];\n",
    "            all_cons = [];\n",
    "            all_slopes = [];\n",
    "\n",
    "            for i in ok:\n",
    "\n",
    "                #######\n",
    "                # get structure, metadata, etc\n",
    "                #######\n",
    "                curr_cell = jointList[i]\n",
    "                curr_metr = curr_cell['metrics']['%s' % mtr];\n",
    "                curr_meta = curr_cell['metadata'];\n",
    "                curr_cons = curr_meta['stimVals'][1];\n",
    "                curr_byDisp = curr_meta['val_con_by_disp'];\n",
    "                if d < len(curr_byDisp):\n",
    "                    curr_inds = curr_byDisp[d];\n",
    "                else:\n",
    "                    continue; # i.e. this dispersion isn't ther\n",
    "\n",
    "                curr_conVals = np.array(curr_cons[curr_inds]);\n",
    "                curr_metrVal = np.array(curr_metr[d, curr_inds]);\n",
    "\n",
    "                #######\n",
    "                # slope measure\n",
    "                # get a slope for how the metric changes with dispersion...mostly just interested in sign (+/-)\n",
    "                #######\n",
    "                non_nan = ~np.isnan(curr_metrVal);\n",
    "                if np.array_equal(np.unique(non_nan), [False]): # i.e. all are nan\n",
    "                    curr_slope = np.nan\n",
    "                else:\n",
    "                    curr_slope = linregress(np.log2(curr_conVals[non_nan]), curr_metrVal[non_nan])[0];\n",
    "\n",
    "                all_sfs.append(curr_metrVal);\n",
    "                all_cons.append(curr_conVals);\n",
    "                all_slopes.append(curr_slope);\n",
    "\n",
    "                #######\n",
    "                # now plot!\n",
    "                #######\n",
    "\n",
    "                ax[d, 0].semilogx(curr_conVals, curr_metrVal, linestyle='--', alpha=0.3, color=clr);\n",
    "                ax[d, 0].set_title('metric: %s' % mtr)\n",
    "\n",
    "            #######\n",
    "            # gather for everything, plot\n",
    "            #######\n",
    "#             if np.array_equal(all_cons, []):\n",
    "#                 continue; # what does this mean? When we subset the data, not all dispersions are\n",
    "            full_slope.append(np.array(all_slopes));\n",
    "\n",
    "            # now take median, save\n",
    "            stacked_cons = np.hstack([x for x in all_cons]);\n",
    "            unique_cons = np.unique(stacked_cons);\n",
    "            mdn = [];\n",
    "            for c_lvl in unique_cons:\n",
    "                ind_byCell = [np.where(x==c_lvl)[0] for x in all_cons]\n",
    "                sf_byCell = np.array([x[i] for x, i in zip(all_sfs, ind_byCell)]);\n",
    "                valids = np.hstack([x for x in sf_byCell]);\n",
    "                mdn.append(np.nanmedian(valids));\n",
    "\n",
    "            # then plot the median!\n",
    "            ax[d, 0].semilogx(unique_cons, mdn, '-', linewidth=4, color=clr);\n",
    "\n",
    "    #         clean up plot\n",
    "            sns.despine(offset=10);\n",
    "            ax[d, 0].set_xlabel('contrast');\n",
    "            ax[d, 0].set_ylabel('%s' % mtr);\n",
    "\n",
    "\n",
    "            #######\n",
    "            # now plot hist of slopes\n",
    "            #######\n",
    "            cleaned = np.clip(hf.nan_rm(np.array(all_slopes)), -2, 2);\n",
    "            ax[d, 1].hist(cleaned, color=clr);\n",
    "            ahh = np.nanmedian(all_slopes);\n",
    "            ax[d, 1].plot(ahh, -1, 'v', color=clr, markersize=10, label='mdn: (%.2f) (disp: %s)' % (ahh, d+1))\n",
    "            if prev_ahh is not None:\n",
    "                ax[d, 1].plot(prev_ahh, -1, 'v', color=prev_clr, markersize=10)\n",
    "            ax[d, 1].set_xlabel('slope value')\n",
    "            ax[d, 1].set_ylabel('# cells')\n",
    "            ax[d, 1].legend();\n",
    "            ax[d, 1].set_title('slope of metric vs. contrast')\n",
    "            prev_ahh = ahh;\n",
    "            prev_clr = clr;\n",
    "\n",
    "        f.suptitle('tuning measure vs. contrast across dispersion %s' % sbst);\n",
    "\n",
    "    #######\n",
    "    # save\n",
    "    #######\n",
    "\n",
    "#     if not os.path.exists(save_loc):\n",
    "#         os.makedirs(save_loc)\n",
    "#     pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "#     pdfSv.savefig(f) # only one figure here...\n",
    "#     pdfSv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preferred spatial frequency with contrast *and* dispersion\n",
    "\n",
    "Before we ask \"how correlated are the frequency shifts at different dispersion levels\", let's first consider the central/preferred frequency as a function of dispersion alone, i.e. start with the constituent parts of the ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms/trajectories of metrics with dispersion, split by contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# First, decide which measure we're using; prepare plot\n",
    "#######\n",
    "sfMetrics = ['sfCom', 'pSf', 'sfComCut'];\n",
    "nMetrics = len(sfMetrics);\n",
    "metricColor = ['k', 'b', 'g']\n",
    "con_tol = 0.02;\n",
    "slope_clip = [-2, 2]; # we're clipping the slope measure for better visualization; see \"slope\" below\n",
    "\n",
    "\n",
    "con_lvls = [0.33, 0.47, 0.68, 1.00];\n",
    "\n",
    "allCon_sf  = dict();\n",
    "allCon_con = dict();\n",
    "\n",
    "for con_ind in range(len(con_lvls)):\n",
    "\n",
    "    con_lvl = con_lvls[con_ind];\n",
    "    saveName = 'sf_disp_con%03d.pdf' % (100*con_lvl)\n",
    "\n",
    "    # how to handle contrast? for each cell, find the contrast that is valid for that dispersion and matches the con_lvl\n",
    "    # to within some tolerance (e.g. +/- 0.01, i.e. 1% contrast)\n",
    "\n",
    "\n",
    "    #######\n",
    "    # set up plot, analyses\n",
    "    #######\n",
    "    f, ax = plt.subplots(nMetrics, 2, figsize=(2*20, nMetrics*20));\n",
    "\n",
    "    full_slope = [];\n",
    "\n",
    "    allCon_sf[con_ind] = dict();\n",
    "    allCon_con[con_ind] = dict();\n",
    "    \n",
    "    for sfM, ind, clr in zip(sfMetrics, range(nMetrics), metricColor):\n",
    "\n",
    "        all_sfs = [];\n",
    "        all_cons = [];\n",
    "        all_slopes = [];\n",
    "\n",
    "        for i in range(len(jointList)):\n",
    "\n",
    "            #######\n",
    "            # get structure, metadata, etc\n",
    "            #######\n",
    "            curr_cell = jointList[i]\n",
    "            curr_sfs = curr_cell['metrics']['%s' % sfM];\n",
    "            curr_meta = curr_cell['metadata'];\n",
    "            curr_disps = curr_meta['stimVals'][0];\n",
    "            curr_cons = curr_meta['stimVals'][1];\n",
    "            curr_byDisp = curr_meta['val_con_by_disp'];\n",
    "\n",
    "            curr_metric = []; # psf or c.o.m.\n",
    "            curr_con    = []; # what's the contrast?\n",
    "\n",
    "            n_disp = len(curr_disps)\n",
    "\n",
    "            for d in range(n_disp):\n",
    "\n",
    "                #######\n",
    "                # get contrast, metric\n",
    "                #######\n",
    "                val_cons = curr_cons[curr_byDisp[d]];\n",
    "                try:\n",
    "                    # if there's no match, then we'll go to the except\n",
    "                    match_ind = np.where(np.abs(val_cons-con_lvl)<=con_tol)[0][0];\n",
    "                    full_con_ind = curr_byDisp[d][match_ind];\n",
    "                    sf, con = curr_sfs[d][full_con_ind], val_cons[match_ind];\n",
    "                except: # if there isn't a match\n",
    "                    sf, con = np.nan, np.nan\n",
    "\n",
    "                curr_con.append(con)\n",
    "                curr_metric.append(sf);          \n",
    "\n",
    "            #######\n",
    "            # slope measure\n",
    "            # get a slope for how the metric changes with dispersion...mostly just interested in sign (+/-)\n",
    "            #######\n",
    "            non_nan = ~np.isnan(curr_metric);\n",
    "            if np.array_equal(np.unique(non_nan), [False]): # i.e. all are nan\n",
    "                curr_slope = np.nan\n",
    "            else:\n",
    "                curr_slope = linregress(np.arange(n_disp)[non_nan], np.array(curr_metric)[non_nan])[0];\n",
    "\n",
    "            all_sfs.append(curr_metric);\n",
    "            all_cons.append(curr_con);\n",
    "            all_slopes.append(curr_slope);\n",
    "\n",
    "            #######\n",
    "            # now plot!\n",
    "            #######\n",
    "\n",
    "            n_disps = len(curr_metric);\n",
    "            xvals = 1+np.arange(n_disps);\n",
    "            ax[ind, 0].semilogy(xvals, curr_metric, '-', color=clr, alpha=0.6);\n",
    "            ax[ind, 0].set_title('metric: %s' % sfM)\n",
    "\n",
    "        #######\n",
    "        # gather for everything, plot\n",
    "        #######\n",
    "        full_slope.append(np.array(all_slopes));\n",
    "\n",
    "        # now take average, save\n",
    "        max_disp = np.max([len(x) for x in all_sfs])\n",
    "        mdn = [];\n",
    "        for d in range(max_disp):\n",
    "            mdn.append(np.nanmedian([asf[d] if len(asf) > d else np.nan for asf in all_sfs]))\n",
    "\n",
    "        allCon_sf[con_ind][sfM] = dict();\n",
    "        allCon_sf[con_ind][sfM]['full'] = all_sfs;\n",
    "        allCon_sf[con_ind][sfM]['median'] = mdn;\n",
    "        allCon_con[con_ind][sfM] = dict();\n",
    "        allCon_con[con_ind][sfM] = all_cons;\n",
    "         \n",
    "        # then plot!\n",
    "        ax[ind, 0].semilogy(1+np.arange(max_disp), mdn, 'r-')\n",
    "        ax[ind, 0].set_xticks(1+np.arange(max_disp))\n",
    "\n",
    "        # clean up plot\n",
    "        sns.despine(offset=10);\n",
    "        ax[ind, 0].set_xlabel('dispersion');\n",
    "        ax[ind, 0].set_ylabel('%s (c/deg)' % sfM);\n",
    "\n",
    "        \n",
    "    #######\n",
    "    # now plot hist of slopes\n",
    "    #######\n",
    "    cleaned = [np.clip(hf.nan_rm(x), -2, 2) for x in full_slope];\n",
    "    ax[0, 1].hist([x for x in cleaned], stacked=True, label=sfMetrics, color=metricColor);\n",
    "    # ax[0, 1].hist([hf.nan_rm(np.array(x)) for x in full_slope], stacked=True, label=sfMetrics, color=metricColor);\n",
    "    # ahh = [np.median(hf.nan_rm(np.array(x))) for x in full_slope];\n",
    "    ahh = [np.nanmedian(x) for x in full_slope];\n",
    "    [ax[0, 1].plot(x, -1, 'v', color=c, markersize=10, label='mdn: %s (%.2f)' % (lbl, x)) for x, c, lbl in zip(ahh, metricColor, sfMetrics)]\n",
    "    ax[0, 1].set_xlabel('slope value')\n",
    "    ax[0, 1].set_ylabel('# cells')\n",
    "    ax[0, 1].legend();\n",
    "    ax[0, 1].set_title('slope of metric vs. dispersion')\n",
    "\n",
    "    #######\n",
    "    # now plot hist of slopes - ONLY for cells with both metrics valid\n",
    "    #######\n",
    "    non_nan = np.logical_and(~np.isnan(full_slope[0]), ~np.isnan(full_slope[1]))\n",
    "    clipped = [np.clip(x[non_nan], -2, 2) for x in full_slope];\n",
    "    ax[1, 1].hist([x for x in clipped], stacked=True, label=sfMetrics, color=metricColor);\n",
    "    # ax[1, 1].hist([x[non_nan] for x in full_slope], stacked=True, label=sfMetrics, color=metricColor);\n",
    "    ahh = [np.median(x[non_nan]) for x in full_slope];\n",
    "    [ax[1, 1].plot(x, -1, 'v', color=c, markersize=10, label='mdn: %s (%.2f)' % (lbl, x)) for x, c, lbl in zip(ahh, metricColor, sfMetrics)]\n",
    "    ax[1, 1].set_xlabel('slope value')\n",
    "    ax[1, 1].set_ylabel('# cells')\n",
    "    ax[1, 1].legend();\n",
    "    ax[1, 1].set_title('matched-only slope of metric vs. dispersion')\n",
    "\n",
    "    f.suptitle('Center SF vs. dispersion for total contrast %.0f%%' % (100*con_lvl));\n",
    "\n",
    "    #######\n",
    "    # save\n",
    "    #######\n",
    "\n",
    "    if not os.path.exists(save_loc):\n",
    "        os.makedirs(save_loc)\n",
    "    pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "    pdfSv.savefig(f) # only one figure here...\n",
    "    pdfSv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now plot median by contrast+dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfMetricsOrig = sfMetrics\n",
    "sfMetrics = ['sfCom', 'pSf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sfCOM and preferred SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = 'sf_disp_summary.pdf'\n",
    "\n",
    "nRow, nCol = 3, 2;\n",
    "\n",
    "f, ax = plt.subplots(nRow, nCol, figsize=(nCol*15, nRow*15), sharey=True, sharex=True)\n",
    "titles = ['full data', 'V1_orig', 'V1 (new)']\n",
    "\n",
    "# what are the other rows??? well, we should split the data into V1_orig, V1 separately \n",
    "# just to make sure nothing of interest is happening in the two experiments\n",
    "\n",
    "for nr, lbl in zip(range(nRow), titles):\n",
    "\n",
    "    for k, ind in zip(sfMetrics, range(len(sfMetrics))):\n",
    "\n",
    "        for c in allCon_sf.keys():\n",
    "            # need to organize the data and access subsets\n",
    "            expInd = np.array([jointList[i]['metadata']['expInd'] for i in range(len(jointList))]);\n",
    "            if nr == 0: # all data\n",
    "                # NOTE: yes, we could just grab 'median', but want to track # cells\n",
    "                ok = np.where(expInd > 0)[0]; # i.e. any experiment...\n",
    "            elif nr == 1: # V1_orig\n",
    "                ok = np.where(expInd == 1)[0]\n",
    "            elif nr == 2: # V1\n",
    "                ok = np.where(expInd != 1)[0]\n",
    "            # compute the median from the relevant subsets, if needed\n",
    "            subset = np.array(allCon_sf[c][k]['full'])[ok]\n",
    "            max_disp = np.max([len(x) for x in subset])\n",
    "            metric, nMed = [], [];\n",
    "            for d in range(max_disp):\n",
    "                metric.append(np.nanmedian([curr[d] if len(curr) > d else np.nan for curr in subset]))\n",
    "                nMed.append(np.sum(~np.isnan([curr[d] if len(curr) > d else np.nan for curr in subset])))\n",
    "                \n",
    "            ax[nr, ind].semilogy(1+np.arange(len(metric)), metric, label='con %.0f%% (n=%s)' % (100*con_lvls[c], nMed));\n",
    "            ax[nr, ind].set_xticks(1+np.arange(len(metric)))\n",
    "\n",
    "        if ind == 0:\n",
    "            ax[nr, ind].set_xlabel('dispersion');\n",
    "            ax[nr, ind].set_ylabel('SF (c/deg)');\n",
    "\n",
    "        ax[nr, ind].legend();\n",
    "        ax[nr, ind].set_title('metric: %s --- %s' % (k, lbl));\n",
    "        sns.despine(ax=ax[nr, ind], offset=10)\n",
    "    \n",
    "f.suptitle('Median SF measure for total contrast X dispersion')\n",
    "    \n",
    "#######\n",
    "# save\n",
    "#######\n",
    "\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments on dispersion changes\n",
    "\n",
    "What can we take away from this? As expected, preferred spatial frequency is higher at higher contrasts (_note: this is on average, as here we are not looking at the shifts within cells_); pSF appears to reduce with increasing dispersion (for a fixed contrast). For SF center of mass (c.o.m.), contrast has a minor effect, particularly as viewed for the newest experiments. With increasing dispersion, c.o.m. tends to stay constant (original stimulus) or increase (newer stimuli).\n",
    "\n",
    "__What do we make of the difference between preferred spatial frequency and center of mass?__ First, let's consider that the set of spatial frequencies is not identical for all dispersions. Specifically, for the new experiment (where the center-of-mass X dispersion interaction is strongest), the extreme spatial frequencies (i.e. lowest/highest) are removed in subsequent dispersion levels. To ensure that the effect above is not caused solely by this stimulus design, let's re-evaluate the c.o.m., but using the same spatial frequencies for all disperesions.\n",
    "\n",
    "TODO: \n",
    "\n",
    "- find examples of interesting or representative C.o.M. versus prefSf cases  \n",
    "- make \"trajectory\" plots for _contrast_. That is, show the trend in pSF/COM/BW/Var within cell X dispersion, and across contrast  \n",
    "- now move on to contrast-shifts across dispersion (i.e. ratio of X for single gratings, Y for dispersion 2, z for dispersion N) -- but not just for the 1:0.33 case, which we already have (see below)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sfCom and sfComCut\n",
    "\n",
    "As described above, let's compare the c.o.m. metric evaluated over all spatial frequencies to just the set of common sfs which appear in all dispersions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfMetrics = ['sfCom', 'sfComCut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = 'sf_disp_comCut.pdf'\n",
    "\n",
    "nRow, nCol = 3, 2;\n",
    "\n",
    "f, ax = plt.subplots(nRow, nCol, figsize=(nCol*15, nRow*15), sharey=True, sharex=True)\n",
    "titles = ['full data', 'V1_orig', 'V1 (new)']\n",
    "\n",
    "# what are the other rows??? well, we should split the data into V1_orig, V1 separately \n",
    "# just to make sure nothing of interest is happening in the two experiments\n",
    "\n",
    "for nr, lbl in zip(range(nRow), titles):\n",
    "\n",
    "    for k, ind in zip(sfMetrics, range(len(sfMetrics))):\n",
    "\n",
    "        for c in allCon_sf.keys():\n",
    "            # need to organize the data and access subsets\n",
    "            expInd = np.array([jointList[i]['metadata']['expInd'] for i in range(len(jointList))]);\n",
    "            if nr == 0: # all data\n",
    "                # NOTE: yes, we could just grab 'median', but want to track # cells\n",
    "                ok = np.where(expInd > 0)[0]; # i.e. any experiment...\n",
    "            elif nr == 1: # V1_orig\n",
    "                ok = np.where(expInd == 1)[0]\n",
    "            elif nr == 2: # V1\n",
    "                ok = np.where(expInd != 1)[0]\n",
    "            # compute the median from the relevant subsets, if needed\n",
    "            subset = np.array(allCon_sf[c][k]['full'])[ok]\n",
    "            max_disp = np.max([len(x) for x in subset])\n",
    "            metric, nMed = [], [];\n",
    "            for d in range(max_disp):\n",
    "                metric.append(np.nanmedian([curr[d] if len(curr) > d else np.nan for curr in subset]))\n",
    "                nMed.append(np.sum(~np.isnan([curr[d] if len(curr) > d else np.nan for curr in subset])))\n",
    "                \n",
    "            ax[nr, ind].semilogy(1+np.arange(len(metric)), metric, label='con %.0f%% (n=%s)' % (100*con_lvls[c], nMed));\n",
    "            ax[nr, ind].set_xticks(1+np.arange(len(metric)))\n",
    "\n",
    "        if ind == 0:\n",
    "            ax[nr, ind].set_xlabel('dispersion');\n",
    "            ax[nr, ind].set_ylabel('SF (c/deg)');\n",
    "\n",
    "        ax[nr, ind].legend();\n",
    "        ax[nr, ind].set_title('metric: %s --- %s' % (k, lbl));\n",
    "        sns.despine(ax=ax[nr, ind], offset=10)\n",
    "    \n",
    "f.suptitle('Median SF measure for total contrast X dispersion')\n",
    "    \n",
    "#######\n",
    "# save\n",
    "#######\n",
    "\n",
    "if not os.path.exists(save_loc):\n",
    "    os.makedirs(save_loc)\n",
    "pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "pdfSv.savefig(f) # only one figure here...\n",
    "pdfSv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the increase in center of mass is not nearly as prominent with only the restricted set of SFs (right). To be fair, the smaller set of SFs provides a metric which will overall be less liable to change. _Nonetheless, it appears that the magnitude - if not existence/sign - of the increased center of mass with increasing contrast is somewhat caused by the stimulus differences between dispersions._ Thus, it would be wise to think carefully about how we can interpret analyses that compare the tuning properties across dispersion, at least for the newer experiments. With the original version of the experiment, we have a consistent set of contrasts and spatial frequency centers. Thus, for those recordings, we can more readily interpret tuning changes across dispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrast shifts across dispersion\n",
    "\n",
    "How correlated are contrast shifts across dispersion? That is, if a cell has a large pSF reduction with contrast for single gratings, is it there for mixture stimuli, too?\n",
    "\n",
    "For this, let's look only at the original experiment, since only two dispersions have 33% contrast in the newer stimulus sets. Why is that a problem? Well, we don't want to correlate a tuning shift across only two dispersions, and if we use a higher contrast (e.g. 67, which will appear in more dispersions), then the shifts will be more difficult to see, anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pSfInd, comInd, bwHalfInd, bw34Ind, varInd = 2, 4, 0, 1, 3; # index into the diffsAtThirdCon array\n",
    "metrInds = [pSfInd, comInd, bwHalfInd, bw34Ind, varInd];\n",
    "metrLabel = ['prefSf', 'c.o.m.', 'bwHalfs', 'bw34', 'sfVar'];\n",
    "\n",
    "nCells = len(jointList);\n",
    "slopes, intercepts, r_vals, p_vals, errs = np.nan*np.zeros(nCells, ), np.nan*np.zeros(nCells, ), np.nan*np.zeros(nCells, ), np.nan*np.zeros(nCells, ), np.nan*np.zeros(nCells, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRow, nCol = len(metrInds), 2;\n",
    "f, ax  = plt.subplots(nRow, nCol, figsize=(10*nCol, 10*nRow));\n",
    "\n",
    "for metrInd, mtr, ind in zip(metrInds, metrLabel, range(len(metrInds))):\n",
    "    for i in range(nCells):\n",
    "        if jointList[i]['metadata']['expInd'] != 1: # only considering original experiment\n",
    "            continue;\n",
    "        \n",
    "        metric = np.array(jointList[i]['metrics']['diffsAtThirdCon'][:, metrInd]);\n",
    "        fullDisp = 1+np.arange(len(metric));\n",
    "        nonNan = np.where(~np.isnan(metric))[0];\n",
    "        if not np.array_equal(nonNan, []):\n",
    "            slopes[i], intercepts[i], r_vals[i], p_vals[i], errs[i] = linregress(fullDisp[nonNan], metric[nonNan]);\n",
    "#             print('cell %d, r=%.2f (p=%.2f)' % (i+1, r_vals[i], p_vals[i]));\n",
    "\n",
    "            ax[ind, 0].plot(fullDisp, metric, 'ko-', alpha=0.3)\n",
    "            ax[ind, 0].set_xlabel('dispersion')\n",
    "            ax[ind, 0].set_ylabel('log2 ratio')\n",
    "            # compute significance\n",
    "            num_nonNan = len(hf.nan_rm(p_vals));\n",
    "            num_sig = np.sum(hf.nan_rm(p_vals)<0.05)\n",
    "            ax[ind, 0].set_title('metric: %s (%d of %d p<0.05)' % (mtr, num_sig, num_nonNan))\n",
    "            \n",
    "    # NOW, plot distribution of correlations across dispersion\n",
    "    cts = ax[ind, 1].hist(hf.nan_rm(r_vals), color='k')[0];\n",
    "    medn, abs_medn = np.median(hf.nan_rm(r_vals)), np.median(hf.nan_rm(np.abs(r_vals)));\n",
    "    ax[ind, 1].set_xlabel('r value');\n",
    "    ax[ind, 1].set_ylabel('# cells');\n",
    "    ax[ind, 1].plot(medn, 1.1*np.max(cts), 'kv', label='mdn r (%.2f)' % (medn), markersize=10)\n",
    "    ax[ind, 1].plot(abs_medn, 1.1*np.max(cts), 'kv', alpha=0.3, label='mdn of abs(r) (%.2f)' % abs_medn, markersize=10)\n",
    "    ax[ind, 1].set_title('correlation across dispersion')\n",
    "    ax[ind, 1].legend();\n",
    "    \n",
    "sns.despine(offset=5);\n",
    "f.suptitle('How correlated are contrast shifts across dispersion?');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot shift distributions at each dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pSfInd, comInd, bwHalfInd, bw34Ind, varInd = 2, 4, 0, 1, 3; # index into the diffsAtThirdCon array\n",
    "metrInds = [pSfInd, comInd, bwHalfInd, bw34Ind, varInd];\n",
    "metrLabel = ['prefSf', 'c.o.m.', 'bwHalfs', 'bw34', 'sfVar'];\n",
    "metrXlabel = ['log2 sf', 'log2 sf', 'oct bw', 'oct bw', 'a.u.']\n",
    "metrBins  = [[-1.5, 1.5], [-1.5, 1.5], [-1.5, 1.5], [-0.5, 0.5], [-0.5, 0.5]]\n",
    "metrColor = ['r', 'r', 'b', 'b', 'b']\n",
    "\n",
    "nCells = len(jointList);\n",
    "maxDisp = np.max([len(jointList[i]['metadata']['stimVals'][0]) for i in range(nCells)]) # 0 for disps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful function for getting the metric value in list comprehension: remember that some \n",
    "def get_metric(jointList, cell, disp, ind):\n",
    "    try: \n",
    "        return jointList[cell]['metrics']['diffsAtThirdCon'][d, metrInd] \n",
    "    except:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### ensure X is same within a column\n",
    "\n",
    "# repeat 3 times - one for all data, one for V1_orig, one for V1 (new)\n",
    "\n",
    "subset = ['all', 'V1orig', 'V1new']\n",
    "\n",
    "for i in range(len(subset)):\n",
    "    \n",
    "    saveName = 'sfTuning_hist_disp_%s.pdf' % subset[i];\n",
    "    \n",
    "    nRow, nCol = maxDisp, len(metrInds);\n",
    "    f, ax  = plt.subplots(nRow, nCol, figsize=(15*nCol, 15*nRow));\n",
    "\n",
    "    for metrInd, mtr, mtrX, mtrC, bnds, ind in zip(metrInds, metrLabel, metrXlabel, metrColor, metrBins, range(len(metrInds))):\n",
    "\n",
    "        mdn_prev = None;\n",
    "\n",
    "        for d in range(maxDisp):\n",
    "            # first, determine which cells we'll consider\n",
    "            expInds = np.array([jointList[i]['metadata']['expInd'] for i in range(nCells)])\n",
    "            if i == 0: # all data\n",
    "                # NOTE: yes, we could just grab 'median', but want to track # cells\n",
    "                ok = np.where(expInds > 0)[0]; # i.e. any experiment...\n",
    "            elif i == 1: # V1_orig\n",
    "                ok = np.where(expInds == 1)[0]\n",
    "            elif i == 2: # V1\n",
    "                ok = np.where(expInds != 1)[0]\n",
    "\n",
    "            # gather the values\n",
    "            vals = np.array([get_metric(jointList, i, d, metrInd) for i in range(nCells)]);               \n",
    "            vals = vals[ok];\n",
    "            mdn = np.median(hf.nan_rm(vals));\n",
    "            vals_clipped = np.clip(hf.nan_rm(vals), bnds[0], bnds[1]);\n",
    "            counts = ax[d, ind].hist(vals_clipped, bins=np.linspace(bnds[0], bnds[1], 11), rwidth=0.8, color=mtrC)[0];\n",
    "            ax[d, ind].set_xlim(bnds)\n",
    "            ax[d, ind].axvline(0.0, color='k', linestyle='--')\n",
    "            ax[d, ind].plot(mdn, 1.1*np.max(counts), 'v', color=mtrC, markersize=10, label='mdn %.2f' % mdn)\n",
    "            if mdn_prev is not None:\n",
    "                ax[d, ind].plot(mdn_prev, 1.1*np.max(counts), 'v', color=mtrC, alpha=0.3, markersize=10)\n",
    "            ax[d, ind].set_title('metric %s, disp %d: (n=%d)' % (mtr, d+1, len(vals_clipped)))\n",
    "            ax[d, ind].legend();\n",
    "            # compute significance\n",
    "#             ax[d, ind].set_title('metric: %s (%d of %d p<0.05)' % (mtr, num_sig, num_nonNan))\n",
    "            ax[d, ind].set_xlabel('%s' % mtrX)\n",
    "            mdn_prev = mdn;\n",
    "\n",
    "    sns.despine(offset=10);\n",
    "    f.suptitle('Differences or ratios in tuning curve metrics [%s]' % subset[i]);\n",
    "\n",
    "    # now save\n",
    "    if not os.path.exists(save_loc):\n",
    "        os.makedirs(save_loc)\n",
    "    pdfSv = pltSave.PdfPages(save_loc + saveName);\n",
    "    pdfSv.savefig(f) # only one figure here...\n",
    "    pdfSv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superposition analysis\n",
    "#### How well do low-level tuning parameters explain the meausres we care about?\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: We have moved regression analysis to V1_clustering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the superposition measures we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "supr_index = [jointList[x]['superpos']['supr_index'] if jointList[x]['superpos'] is not None else np.nan for x in np.arange(len(jointList))]\n",
    "corr_err = [jointList[x]['superpos']['corr_derivWithErr'] if jointList[x]['superpos'] is not None else np.nan for x in np.arange(len(jointList))]\n",
    "corr_errNorm = [jointList[x]['superpos']['corr_derivWithErrNorm'] if jointList[x]['superpos'] is not None else np.nan for x in np.arange(len(jointList))]\n",
    "errs_auc = [jointList[x]['superpos']['errs_auc'] if jointList[x]['superpos'] is not None else np.nan for x in np.arange(len(jointList))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cells = np.logical_and(np.logical_and(~np.isnan(corr_err), ~np.isnan(corr_errNorm)), ~np.isnan(supr_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, get the basic tuning measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: The following will be stored as log2 (ratios, frequency values)\n",
    "# - f1f0 (ratio)\n",
    "# - c50 (contrast viewed in log)\n",
    "# - sfPref_basic (sf viewed in log2)\n",
    "# - tfPref (tf viewed in log2)\n",
    "# - mix_sfPref (sf measure)\n",
    "# - mix_charFreq (sf measure)\n",
    "# - gsf_data/mod (size in degrees)\n",
    "# - mix_prefRadData (already in log2)\n",
    "# - mix_psfModRat[Norm] (already in log2)\n",
    "\n",
    "nCells = len(jointList);\n",
    "\n",
    "c50_basic = np.nan * np.zeros((nCells, ))\n",
    "c50_eff_basic = np.copy(c50_basic);\n",
    "\n",
    "f1f0 = np.copy(c50_basic)\n",
    "\n",
    "supr_data = np.copy(c50_basic)\n",
    "supr_mod = np.copy(c50_basic)\n",
    "gsf_data = np.copy(c50_basic)\n",
    "gsf_mod = np.copy(c50_basic)\n",
    "\n",
    "sfPref_basic = np.copy(c50_basic)\n",
    "sfBW_basic = np.copy(c50_basic)\n",
    "\n",
    "tfPref = np.copy(c50_basic)\n",
    "tfBW = np.copy(c50_basic)\n",
    "\n",
    "oriPref = np.copy(c50_basic)\n",
    "oriBW = np.copy(c50_basic)\n",
    "oriCV = np.copy(c50_basic)\n",
    "oriDS = np.copy(c50_basic)\n",
    "\n",
    "mix_sfPref = np.copy(c50_basic)\n",
    "mix_c50 = np.copy(c50_basic)\n",
    "mix_c50_eff = np.copy(c50_basic)\n",
    "mix_bwHalf = np.copy(c50_basic)\n",
    "mix_bw34 = np.copy(c50_basic)\n",
    "mix_charFreq = np.copy(c50_basic)\n",
    "\n",
    "mix_prefRatData = np.copy(c50_basic)\n",
    "mix_bwHalfDiffData = np.copy(c50_basic)\n",
    "mix_bw34DiffData = np.copy(c50_basic)\n",
    "\n",
    "mix_psfModRat = np.copy(c50_basic)\n",
    "mix_psfModRat_norm = np.copy(c50_basic)\n",
    "\n",
    "for ind in np.arange(nCells):\n",
    "    #########\n",
    "    ### Basics (basic characterizations, f1f0)\n",
    "    #########\n",
    "    # f1/f0\n",
    "    try:\n",
    "        f1f0[ind] = np.log2(jointList[ind]['metrics']['f1f0_ratio']);\n",
    "    except:\n",
    "        pass\n",
    "    # RF SIZE\n",
    "    try: # suppression index from model fit (Cavanaugh '02, see hf.sizeTune for details)\n",
    "        supr_data[ind] = jointList[ind]['basics']['rfsize']['suprInd_data']\n",
    "    except:\n",
    "        pass\n",
    "    try: # suppression index from data (Cavanaugh '02, see hf.sizeTune for details)\n",
    "        supr_mod[ind] = jointList[ind]['basics']['rfsize']['suprInd_model']\n",
    "    except:\n",
    "        pass\n",
    "    try: # grating summation field from model fit (Cavanaugh '02, see hf.sizeTune for details)\n",
    "        gsf_data[ind] = np.log2(jointList[ind]['basics']['rfsize']['gsf_data'])\n",
    "    except:\n",
    "        pass\n",
    "    try: # rating summation fieldfrom data (Cavanaugh '02, see hf.sizeTune for details)\n",
    "        gsf_mod[ind] = np.log2(jointList[ind]['basics']['rfsize']['gsf_model'])\n",
    "    except:\n",
    "        pass\n",
    "    # RVC\n",
    "    try: # get c50\n",
    "        c50_basic[ind] = np.log2(jointList[ind]['basics']['rvc']['c50'])\n",
    "        c50_eff_basic[ind] = np.log2(jointList[ind]['basics']['rvc']['c50_eval'])\n",
    "    except:\n",
    "        pass\n",
    "    # TF\n",
    "    try: # get prefTF\n",
    "        tfPref[ind] = np.log2(jointList[ind]['basics']['tf']['tfPref'])\n",
    "    except:\n",
    "        pass\n",
    "    try: # get tfBW\n",
    "        tfBW[ind] = jointList[ind]['basics']['tf']['tfBW_oct']\n",
    "    except:\n",
    "        pass\n",
    "    # Orientation\n",
    "    try: # get oriPref\n",
    "        oriPref[ind] = jointList[ind]['basics']['ori']['pref']\n",
    "    except:\n",
    "        pass\n",
    "    try: # get oriBW\n",
    "        oriBW[ind] = jointList[ind]['basics']['ori']['bw']\n",
    "    except:\n",
    "        pass\n",
    "    try: # get oriCV\n",
    "        oriCV[ind] = jointList[ind]['basics']['ori']['cv']\n",
    "    except:\n",
    "        pass\n",
    "    try: # get DS\n",
    "        oriDS[ind] = jointList[ind]['basics']['ori']['DS']\n",
    "    except:\n",
    "        pass\n",
    "    # SF tuning (from the sf1* series, not as measured in the sfMix experiment)\n",
    "    try: # get prefSf\n",
    "        sfPref_basic[ind] = np.log2(jointList[ind]['basics']['sf']['sfPref'])\n",
    "    except:\n",
    "        pass\n",
    "    try: # get sfbw\n",
    "        sfBW_basic[ind] = jointList[ind]['basics']['sf']['sfBW_oct']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    #########\n",
    "    ### sfMixture metrics\n",
    "    #########\n",
    "    try: # prefSf\n",
    "        mix_sfPref[ind] = np.log2(jointList[ind]['metrics']['pSf'][0,-1]);\n",
    "    except:\n",
    "        pass\n",
    "    try: # c50 at SF nearest to prefSF\n",
    "        sf_vals = jointList[ind]['metadata']['stimVals'][2];\n",
    "        if ~np.isnan(mix_sfPref[ind]):\n",
    "            prefSfInd = np.argmin(np.square(np.power(2, mix_sfPref[ind]) - sf_vals));\n",
    "        else:\n",
    "            prefSfInd = np.floor(len(sf_vals)/2);\n",
    "        mix_c50[ind] = np.log2(jointList[ind]['metrics']['c50'][0, prefSfInd]);\n",
    "        mix_c50_eff[ind] = np.log2(jointList[ind]['metrics']['c50_eval'][0, prefSfInd]);\n",
    "    except:\n",
    "        pass\n",
    "    try: # sfBW 1/2\n",
    "        mix_bwHalf[ind] = jointList[ind]['metrics']['bwHalf'][0,-1];\n",
    "    except:\n",
    "        pass\n",
    "    try: # sfBW 3/4\n",
    "        mix_bw34[ind] = jointList[ind]['metrics']['bw34'][0,-1];\n",
    "    except:\n",
    "        pass\n",
    "    try: # characteristic frequency\n",
    "        mix_charFreq[ind] = np.log2(jointList[ind]['metrics']['dog_charFreq'][0,-1]);\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dispInd = 0; # evaluate this for single gratings\n",
    "        # prefSf ratio 1:0.33 in data\n",
    "        mix_prefRatData[ind] = jointList[ind]['metrics']['diffsAtThirdCon'][dispInd, 2];\n",
    "        # bwHalf ratio 1:0.33 in data\n",
    "        mix_bwHalfDiffData[ind] = jointList[ind]['metrics']['diffsAtThirdCon'][dispInd, 0];\n",
    "        # bw34 ratio 1:0.33 in data\n",
    "        mix_bw34DiffData[ind] = jointList[ind]['metrics']['diffsAtThirdCon'][dispInd, 1];\n",
    "    except:\n",
    "        pass\n",
    "    try: # prefSfModRat\n",
    "        dispInd = 0; # evaluate this for single gratings\n",
    "        # we fit a model to the prefSF as a function of contrast - the log2 ratio of that model at the highest/lowest valid contrast\n",
    "        mix_psfModRat[ind] = jointList[ind]['metrics']['pSfModRat'][dispInd, 0];\n",
    "        # the same ratio normalized by the log contrast ratio of highest/lowest\n",
    "        mix_psfModRat_norm[ind] = jointList[ind]['metrics']['pSfModRat'][dispInd, 1];\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ind = np.vstack((f1f0, supr_data, supr_mod, gsf_data, gsf_mod, c50_basic, tfPref, tfBW, oriPref, oriBW, oriCV, oriDS, sfBW_basic, sfPref_basic, mix_sfPref, mix_charFreq, mix_bwHalf, mix_bw34, supr_index))\n",
    "all_dep = np.vstack((mix_bwHalfDiffData, mix_bw34DiffData, mix_prefRatData, mix_psfModRat, mix_psfModRat_norm, corr_err, corr_errNorm, errs_auc))\n",
    "\n",
    "all_ind_names = ['f1f0', 'supr_data', 'supr_mod', 'gsf_data', 'gsf_mod', 'c50', 'tfPref', 'tfBW', 'oriPref', 'oriBW', 'oriCV', 'oriDS', 'sfBW_basic', 'sfPref_basic', 'mix_sfPref', 'mix_charFreq', 'mix_bwHalf', 'mix_bw34', 'supr_index']\n",
    "all_dep_names = ['mix_bwHalfDiffData', 'mix_bw34DiffData', 'mix_prefRatData', 'mix_psfModRat', 'mix_psfModRat_norm', 'corr_err', 'corr_errNorm', 'errs_auc']\n",
    "# some varialbles should be plotted log; these are those\n",
    "all_ind_log_names = ['f1f0', 'gsf_data', 'gsf_mod', 'c50', 'tfPref', 'sfPref_basic', 'mix_sfPref', 'mix_charFreq']\n",
    "all_dep_log_names = ['mix_prefRatData', 'mix_psfModRat', 'mix_psfModRat_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP STOP STOP\n",
    "\n",
    "Are you sure you want to run below this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depricated/to-move\n",
    "Note that the sections below are either obselete or will be (or have been) moved to other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot pSf ratio against exc/norm filter ratio..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the following is only for weighted normalization!\n",
    "# sfRatToUse = psfModRats; # mod ratios or from-data ratios?\n",
    "\n",
    "# c50_loc = 2;\n",
    "\n",
    "# modParamRatio = lambda params: params[0]/np.power(10, params[-2]) if params != [] else np.nan; # it's second to last parameter\n",
    "# modExcNormRat = [modParamRatio(jointList[x]['model']['params_wght']) for x in range(len(jointList))]\n",
    "# getC50 = lambda params: params[c50_loc] if params != [] else np.nan; \n",
    "# c50s = np.array([getC50(jointList[x]['model']['params_wght']) for x in range(len(jointList))]);\n",
    "# zTO1 = lambda x: np.divide(x - np.nanmin(x), np.nanmax(x - np.nanmin(x)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argsort(modExcNormRat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.semilogy(np.clip(modExcNormRat, np.power(10, -2.0), np.power(10, 2)), 'o');\n",
    "# plt.xlabel('cell #');\n",
    "# plt.ylabel('peak SF ratio: exc::norm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_inds = np.logical_and(~np.isnan(sfRatToUse), ~np.isnan(np.log2(modExcNormRat)));\n",
    "# c50_color = cm.gray(zTO1(np.clip(c50s[val_inds], -2, 1)));\n",
    "# plt.scatter(sfRatToUse[val_inds], np.clip(np.log2(np.array(modExcNormRat)[val_inds]), -2, 2), marker='o', color=c50_color);\n",
    "# plt.xlabel('data contrast shift');\n",
    "# plt.ylabel('model exc/norm ratio');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### STATISTICAL TEST??\n",
    "# nan_rm = lambda x: x[~np.isnan(x)]\n",
    "# ttest_1samp(nan_rm(psfRats), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this is older analyses - still good, but not my newest for VSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height = 3/4; # measure BW at half-height\n",
    "# sf_range = [0.01, 10]; # allowed values of 'mu' for fits - see descr_fit.py for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### sf tuning\n",
    "# descrExp = hf.np_smart_load(data_loc + descrName)\n",
    "# # HACKY: Just setting to descrFits on exp data...\n",
    "# descrMod = hf.np_smart_load(data_loc + descrName)\n",
    "# # descrMod = np.load(data_loc + descrModName, encoding='latin1').item()\n",
    "\n",
    "# ### rvc\n",
    "# rvcExp = hf.np_smart_load(data_loc + rvcName)\n",
    "# # HACKY: Just setting to descrFits on exp data...\n",
    "# rvcMod = hf.np_smart_load(data_loc + rvcName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modParams = np.load(data_loc + fitName, encoding='latin1').item();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bandwidth/prefSf measures organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get experiment parameters/organization\n",
    "# nCells = len(jointList)\n",
    "\n",
    "# maxComp = np.zeros((3,), dtype=int); # i.e.\n",
    "\n",
    "# for f in dataList['unitName']:\n",
    "\n",
    "#     cell = hf.np_smart_load(data_loc + f + '_sfm.npy');\n",
    "#     expInd = hf.get_exp_ind(data_loc, f)[0];\n",
    "#     _, stimVals, val_con_by_disp, _, _ = hf.tabulate_responses(cell, expInd);\n",
    "\n",
    "#     for currComp, i in zip(stimVals, range(len(stimVals))):\n",
    "#         if len(currComp) > maxComp[i]:\n",
    "#             maxComp[i] = len(currComp);\n",
    "\n",
    "# # now we know the maximum number of dispersions, sfs, and contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nFamiliesMax, nConsMax, nSfsMax = maxComp;\n",
    "\n",
    "# bwMod = np.zeros((nCells, nFamiliesMax, nConsMax)) * np.nan;\n",
    "# bwExp = np.zeros((nCells, nFamiliesMax, nConsMax)) * np.nan;\n",
    "\n",
    "# pSfMod = np.zeros((nCells, nFamiliesMax, nConsMax)) * np.nan;\n",
    "# pSfExp = np.zeros((nCells, nFamiliesMax, nConsMax)) * np.nan;\n",
    "\n",
    "# c50Mod = np.zeros((nCells, nFamiliesMax, nSfsMax)) * np.nan\n",
    "# c50Exp = np.zeros((nCells, nFamiliesMax, nSfsMax)) * np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(jointList)):\n",
    "\n",
    "#     curr_cell = jointList[i]\n",
    "    \n",
    "#     for f in range(nFamiliesMax):\n",
    "#         # spatial frequency stuff\n",
    "#         for c in range(nConsMax):\n",
    "        \n",
    "#             try:\n",
    "#                 # on data\n",
    "#                 ignore, bwExp[i, f, c] = hf.compute_SF_BW(descrExp[i]['params'][f, c, :], height, sf_range)\n",
    "#                 pSfExp[i, f, c] = descrExp[i]['params'][f, c, muLoc]\n",
    "#             except: # then this dispersion does not have that contrast value, but it's ok - we already have nan\n",
    "#                 pass \n",
    "\n",
    "#             try:\n",
    "#                 # on model\n",
    "#                 ignore, bwMod[i, f, c] = hf.compute_SF_BW(descrMod[i]['params'][f, c, :], height, sf_range)\n",
    "#                 pSfMod[i, f, c] = descrMod[i]['params'][f, c, muLoc]\n",
    "#             except: # then this dispersion does not have that contrast value, but it's ok - we already have nan\n",
    "#                 pass\n",
    "\n",
    "#         # RVC stuff\n",
    "#         for s in range(nSfs):\n",
    "#             if i in rvcExp:\n",
    "#                 # on data\n",
    "#                 try:\n",
    "#                     c50Exp[i, f, s] = rvcExp[i]['params'][f, s, c50Loc];\n",
    "#                 except: # then this dispersion does not have that SF value, but it's ok - we already have nan\n",
    "#                     pass\n",
    "                \n",
    "#             if i in rvcMod:\n",
    "#                 # on model\n",
    "#                 try:\n",
    "#                     c50Mod[i, f, s] = rvcMod[i]['params'][f, s, c50Loc];\n",
    "#                 except: # then this dispersion does not have that SF value, but it's ok - we already have nan\n",
    "#                     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bwModRats = np.zeros((nCells, nFamilies, nCons, nCons)) * np.nan;\n",
    "# bwExpRats = np.zeros((nCells, nFamilies, nCons, nCons)) * np.nan;\n",
    "\n",
    "# pSfModRats = np.zeros((nCells, nFamilies, nCons, nCons)) * np.nan;\n",
    "# pSfExpRats = np.zeros((nCells, nFamilies, nCons, nCons)) * np.nan;\n",
    "\n",
    "# c50ModRats = np.zeros((nCells, nFamilies, nSfs, nSfs)) * np.nan;\n",
    "# c50ExpRats = np.zeros((nCells, nFamilies, nSfs, nSfs)) * np.nan;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in descrExp:\n",
    "#     for f in range(nFamilies):\n",
    "#         # as function of contrast\n",
    "#         for comb in itertools.combinations(range(nCons), 2):\n",
    "#             bwModRats[i,f,comb[0],comb[1]] = bwMod[i,f,comb[1]] - bwMod[i,f,comb[0]]\n",
    "#             bwExpRats[i,f,comb[0],comb[1]] = bwExp[i,f,comb[1]] - bwExp[i,f,comb[0]]\n",
    "\n",
    "#             pSfModRats[i,f,comb[0],comb[1]] = pSfMod[i,f,comb[1]] / pSfMod[i,f,comb[0]]\n",
    "#             pSfExpRats[i,f,comb[0],comb[1]] = pSfExp[i,f,comb[1]] / pSfExp[i,f,comb[0]]\n",
    "\n",
    "#         # as function of SF\n",
    "#         for comb in itertools.permutations(range(nSfs), 2):\n",
    "#             c50ModRats[i,f,comb[0],comb[1]] = c50Mod[i,f,comb[1]] / c50Mod[i,f,comb[0]]\n",
    "#             c50ExpRats[i,f,comb[0],comb[1]] = c50Exp[i,f,comb[1]] / c50Exp[i,f,comb[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize model-derived ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than taking just the data, we can restrict ourselves to well-fit (i.e. varExpl > a set threshold) tuning curves; read off the variable of interest (e.g. prefSf) for that curve, and fit a model to the progression of that variable with , e.g. contrast -- finally, read off the variable of interest from the model, rather than just the point/discrete values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot + compute pSfRatio in a model-derived way (i.e. use all prefSfs at different cons, fit a model and read off)\n",
    "# # for individual cell\n",
    "\n",
    "# varThresh = 80;\n",
    "# cellNum = 4;\n",
    "# disp = 0;\n",
    "\n",
    "# name = dataList['unitName'][cellNum-1];\n",
    "# expInd = hf.get_exp_ind(data_loc, name)[0];\n",
    "# cell = hf.np_smart_load(data_loc + name + '_sfm.npy')\n",
    "# _, stimVals, val_con_by_disp, _, _ = hf.tabulate_responses(cell, expInd);\n",
    "# all_cons = stimVals[1];\n",
    "# nFamilies = len(stimVals[0]);\n",
    "\n",
    "\n",
    "# pSfRatio, psf_model, opt_params = hf.dog_prefSfMod(descrExp[cellNum-1], allCons=all_con, disp=disp, varThresh=varThresh, dog_model=0)\n",
    "\n",
    "# valInds = np.where(descrExp[cellNum-1]['varExpl'][disp, :] > varThresh)[0];\n",
    "# logConRat = np.log2(all_con[valInds[-1]]/all_con[valInds[0]]);\n",
    "# plt.plot(all_con, psf_model(*opt_params, con=all_con))\n",
    "# plt.plot(all_con[valInds], pSfExp[cellNum-1, disp, valInds], 'o')\n",
    "# plt.title('Ratio at extremes: %.2f, or %.2f per log(2) contrast step' % (pSfRatio, pSfRatio/logConRat));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Compute pSfRatio in a model-derived way (i.e. use all prefSfs at different cons, fit a model and read off)\n",
    "\n",
    "# varThresh = 80;\n",
    "# pSfDerivRats = np.zeros((nCells, nFamilies, 2)) * np.nan\n",
    "\n",
    "# for f, ind in zip(dataList['unitName'], range(nCells)):\n",
    "    \n",
    "#     expInd = hf.get_exp_ind(data_loc, f)[0];\n",
    "#     cell = hf.np_smart_load(data_loc + f + '_sfm.npy')\n",
    "#     _, stimVals, val_con_by_disp, _, _ = hf.tabulate_responses(cell, expInd);\n",
    "#     all_cons = stimVals[1];\n",
    "#     nFamilies = len(stimVals[0]);\n",
    "    \n",
    "#     for disp in range(nFamilies):\n",
    "\n",
    "#         try:\n",
    "#             pSfRatio, psf_model, opt_params = hf.dog_prefSfMod(descrExp[ind], allCons=all_con, disp=disp, varThresh=varThresh, dog_model=0)\n",
    "\n",
    "#             valInds = np.where(descrExp[ind]['varExpl'][disp, :] > varThresh)[0];\n",
    "#             logConRat = np.log2(all_con[valInds[-1]]/all_con[valInds[0]]);\n",
    "\n",
    "#             pSfDerivRats[ind, disp, :] = [pSfRatio, np.log2(pSfRatio)/logConRat]\n",
    "#         except:\n",
    "#             pass # then either fit doesn't exist or no SF descr fits passed thresh; we've already initialized to nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(2, nFamiliesMax, figsize=(10*nFamiliesMax, 12));\n",
    "\n",
    "# binsRat = np.linspace(-4,4,9)\n",
    "# binsUnit = np.linspace(-4,4,9)\n",
    "\n",
    "# for i in range(nFamiliesMax):\n",
    "\n",
    "#     nonNan = np.where(~np.isnan(pSfDerivRats[:, i, 0]))\n",
    "#     ax[0, i].hist(np.clip(np.log2(pSfDerivRats[nonNan, i, 0][0]), binsRat[0], binsRat[-1]), bins=binsRat, rwidth=0.9)\n",
    "#     if i == 0:\n",
    "#         ax[0, i].set_xlabel('Ratio of prefSf at highest to lowest contrast')\n",
    "    \n",
    "#     ax[1, i].hist(np.clip(pSfDerivRats[nonNan, i, 1][0], binsUnit[0], binsUnit[-1]), bins=binsUnit, rwidth=0.9)\n",
    "#     if i == 0:\n",
    "#         ax[1, i].set_xlabel(r'log $\\Delta$ prefSf per unit log contrast')\n",
    "\n",
    "# sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio plots as matricies (i.e. all relevant pairwise ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute all pairwise combinations so that we can compute the ratios/differences of these measures.\n",
    "- For the pSf/BW, which we measure at distinct contrasts, we want to take the ratio as relative to the higher contrast, which is the larger index into all_cons\n",
    "- For the c50 measure, this is a function of SF; we'll eventually want to take the ratios relative to the preferred (or closest-to-preferred) SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pSfExp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pSfExp[ind, 0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now, example \"upper diag plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cellNum = 17;\n",
    "# dispInd = 0;\n",
    "\n",
    "# cell = hf.np_smart_load(data_loc + dataList['unitName'][cellNum-1] + '_sfm.npy');\n",
    "# expInd = hf.get_exp_ind(data_loc, dataList['unitName'][cellNum-1])[0];\n",
    "# _, stimVals, val_con_by_disp, _, _ = hf.tabulate_responses(cell, expInd);\n",
    "# all_disp, all_con, all_sf = stimVals;\n",
    "\n",
    "# metrics = [bwExpRats, pSfExpRats, c50ExpRats];\n",
    "# metricNames = ['bandwidth difference (oct)', 'prefSf ratio', 'c50 ratio'];\n",
    "# metricUnits = ['(con %)', '(con %)', '(sf cpd)']\n",
    "# metricScales = [all_con, all_con, all_sf];\n",
    "# metricLog    = [0, 1, 1]; # do not log bw diff, but log other measures\n",
    "# metricRef    = [0, 1, 1]; # what is the reference value\n",
    "# nPlots = len(metrics);\n",
    "\n",
    "# f, ax = plt.subplots(2, nPlots, figsize=(15*nPlots, 2*12))\n",
    "\n",
    "# for m, mn, ms, mu, ml, mr, ind in zip(metrics, metricNames, metricScales, metricUnits, metricLog, metricRef, range(nPlots)):\n",
    "\n",
    "#     ### First, the overall summaries\n",
    "#     if ml == 1:\n",
    "#         imCurr = ax[0, ind].imshow(np.log2(m[cellNum-1, dispInd, :, :]));\n",
    "#     elif ml == 0:\n",
    "#         imCurr = ax[0, ind].imshow(m[cellNum-1, dispInd, :, :]);\n",
    "#     ax[0, ind].set_xlabel('reference %s' % mu)\n",
    "#     ax[0, ind].set_ylabel('test')\n",
    "#     # find out which are valid indices\n",
    "#     valids = np.where(np.any(~np.isnan(m[cellNum-1, dispInd, :, :]), 0))[0]\n",
    "#     ax[0, ind].xaxis.set_ticks(valids)\n",
    "#     ax[0, ind].yaxis.set_ticks(valids)\n",
    "#     ax[0, ind].xaxis.set_ticklabels(['%.2f' % x for x in ms[valids]], rotation='30')\n",
    "#     ax[0, ind].yaxis.set_ticklabels(['%.2f' % x for x in ms[valids]], rotation='30')\n",
    "#     f.colorbar(imCurr, ax=ax[0, ind]);\n",
    "#     ax[0, ind].set_title('%s' % mn);\n",
    "    \n",
    "#     ### Then, the ratio plots for the \"preferred\" case (i.e. highest con, peakSf)\n",
    "#     # all of the plots here should be w.r.t. log(x)\n",
    "#     if np.all(ms == all_sf): # find peak SF...\n",
    "#         peakSf = pSfExp[ind, dispInd, val_con_by_disp[dispInd][-1]]; # i.e. highest contrast\n",
    "#         peakInd = np.argmin(np.square(ms[valids] - peakSf));\n",
    "#         peakInd = valids[peakInd];\n",
    "#     else:\n",
    "#         peakInd = valids[-1]; # i.e. the highest contrast\n",
    "#     if ml == 1:\n",
    "#         ax[1, ind].loglog(ms[valids], m[cellNum-1, dispInd, valids, peakInd], 'o-')\n",
    "#     elif ml == 0:\n",
    "#         ax[1, ind].semilogx(ms[valids], m[cellNum-1, dispInd, valids, peakInd], 'o-')\n",
    "#     # set the reference line\n",
    "#     ax[1, ind].axhline(mr, linestyle='--')\n",
    "#     # set the axes...\n",
    "#     ax[1, ind].xaxis.set_ticks(ms[valids])\n",
    "#     ax[1, ind].xaxis.set_ticklabels(['%.2f' % x for x in ms[valids]], rotation='30')\n",
    "#     # labels\n",
    "#     ax[1, ind].set_xlabel('test (ref: %.2f) %s' % (ms[peakInd], mu))\n",
    "#     ax[1, ind].set_ylabel('%s (ref:test)' % mn)\n",
    "    \n",
    "# f.suptitle('Changes with contrast: cell %d, dispersion %d' % (cellInd, dispInd));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the old way...\n",
    "# bwDiffMod = bwMod[:, :, conInd[1]] - bwMod[:,:, conInd[0]]\n",
    "# bwDiffExp = bwExp[:, :, conInd[1]] - bwExp[:,:, conInd[0]]\n",
    "\n",
    "# pSfRatioMod = pSfMod[:, :, conInd[1]] / pSfMod[:,:, conInd[0]]\n",
    "# pSfRatioExp = pSfExp[:, :, conInd[1]] / pSfExp[:,:, conInd[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
